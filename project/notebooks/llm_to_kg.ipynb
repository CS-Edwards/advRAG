{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q python-dotenv\n",
    "! pip install -q neo4j\n",
    "! pip install -q langchain\n",
    "! pip install -q langchain-openai\n",
    "! pip install -q tiktoken\n",
    "! pip install -q torch\n",
    "! pip install -q transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv(\"URL\")\n",
    "os.environ[\"NEO4J_USERNAME\"]= os.getenv(\"USERNAME\")\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv(\"PASSWORD2\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDB = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "    parentDocUUID: str,\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "          \"system\",\n",
    "          f\"\"\"# Knowledge Graph Instructions for GPT-3.5 Turbo\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. You are evaluating information from an academic institution, creating a knowledge graph of concepts related to AACSB accreditation.\n",
    "You should be able to identify learning goals (eg. Written Communication, Oral Communication, Critical Thinking, Ethics, Globalization, Information Technology),\n",
    "along with how and when they are assessed.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- ** Provided REQUIRED Property**: Each node must have a the property key \"parentDocUUID\", this is the value {parentDocUUID}. DO NOT CREATE a VALUE only use the provided value.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "          \"\"\"),\n",
    "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    parentDocUUID: str,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(parentDocUUID, nodes, rels)\n",
    "    data = extract_chain.invoke(document.page_content)['function']\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    graphDB.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import_date = datetime.now()\n",
    "path =r'..\\data\\institution\\sample\\GCSU\\appendix\\gcsu_assurance_of_learning.pdf'\n",
    "\n",
    "# Read the wikipedia article\n",
    "raw_documents = PyPDFLoader(path)\n",
    "pages = raw_documents.load_and_split()\n",
    "\n",
    "\n",
    "# Define chunking strategy\n",
    "#text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000, #changed from 2000\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "# # Only take the first the raw_documents\n",
    "## document chunks\n",
    "documents = text_splitter.split_documents(pages[5:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(documents))\n",
    "#print(documents[0].metadata['page'])\n",
    "#print(type(pages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_source_query=\"\"\"\n",
    "MERGE(d:Docsource {source: $docSource})\n",
    "    ON CREATE SET\n",
    "        d.importDate = $importDate,\n",
    "        d.nodeType = 'DOCSOURCE',\n",
    "        d.nodeCat = 'INSTITUTION'\n",
    "RETURN d, elementID(d) as elementId\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# document_query =\"\"\"\n",
    "# MERGE(d:Document {parentDocSourceId: $docSourceId})\n",
    "#    ON CREATE SET\n",
    "#         d.bertSummary = $modelSummary,\n",
    "#         d.standardClassification = $modelClassification,\n",
    "#         d.nodeType = 'DOCUMENT',\n",
    "#         d.nodeCat='INSTITUTION',\n",
    "#         d.pageIdxNum = $page,\n",
    "#         d.source = $source\n",
    "# RETURN d, elementID(d) as elementId\n",
    "# \"\"\"\n",
    "\n",
    "# doc_chunk_query = \"\"\"\n",
    "# MERGE(c:Chunk {parentDocId: $parentDocId})\n",
    "#     ON CREATE SET\n",
    "#         c.text = $documentText,\n",
    "#         c.embedding = NULL,\n",
    "#         c.nodeType = 'DOCUMENTCHUNK',\n",
    "#         c.nodeCat = 'INSTITUTION'\n",
    "# RETURN c, elementID(c) as elementId\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "document_query = \"\"\"\n",
    "CREATE (d:Document {\n",
    "    parentDocSourceId: $docSourceId,\n",
    "    docUUID: apoc.create.uuid(),\n",
    "    modSummary: $modelSummary,\n",
    "    standardClassification: $modelClassification,\n",
    "    nodeType: 'DOCUMENT',\n",
    "    nodeCat: 'INSTITUTION',\n",
    "    sourcePageIdxNum: $page,\n",
    "    source: $source,\n",
    "    inputIdx: $idx\n",
    "})\n",
    "RETURN d, d.docUUID as elementId, ID(d) as idNum\n",
    "\"\"\"\n",
    "\n",
    "doc_chunk_query = \"\"\"\n",
    "CREATE (c:Chunk {\n",
    "    UUID: apoc.create.uuid(),\n",
    "    parentDocId: $parentDocId,\n",
    "    text: $documentText,\n",
    "    embedding: null,\n",
    "    nodeType: 'DOCUMENTCHUNK',\n",
    "    nodeCat: 'INSTITUTION',\n",
    "    sourcePageIdxNum: $page,\n",
    "    source: $source,\n",
    "    inputIdx: $idx\n",
    "})\n",
    "RETURN c, elementId(c) as elementId\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_link_query = \"\"\"\n",
    "MATCH (from_same_section:Document)\n",
    "WHERE from_same_section.parentDocSourceId = $docSourceId\n",
    "WITH from_same_section\n",
    "    ORDER BY from_same_section.inputIdx ASC\n",
    "WITH collect(from_same_section) as doc_list\n",
    "    call apoc.nodes.link(\n",
    "        doc_list,\n",
    "        \"NEXT\",\n",
    "        {avoidDuplicates: true}\n",
    "    )\n",
    "    RETURN size(doc_list)\n",
    "\"\"\"\n",
    "\n",
    "link_doc_chunk_to_parent_doc =\"\"\" \n",
    "MATCH (c:Chunk), (d: Document)\n",
    "WHERE c.parentDocId = $docParentId\n",
    "    AND c.sourcePageIdxNum = d.sourcePageIdxNum\n",
    "    AND c.source = d.source\n",
    "    AND c.inputIdx = d.inputIdx\n",
    "MERGE (c)-[newRelationship:PART_OF]->(d)\n",
    "RETURN count(newRelationship) as num_links_created\n",
    "\"\"\"\n",
    "\n",
    "link_to_source_doc = \"\"\" \n",
    "MATCH(d:Document),(s:Docsource)\n",
    "WHERE d.parentDocSourceId = $docSourceId\n",
    "    AND d.source = s.source\n",
    "MERGE (d)-[newRelationship:SOURCE_FROM]->(s)\n",
    "RETURN count(newRelationship) AS num_links_created\n",
    "\"\"\"\n",
    "\n",
    "link_kg_nodes_to_source_doc = \"\"\" \n",
    "MATCH (n), (d:Document) \n",
    "WHERE d.docUUID = n.parentdocuuid \n",
    "MERGE (n)-[newRelationship: CONTENT_FROM_DOC]->(d) \n",
    "RETURN count(newRelationship)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../utils')  \n",
    "from input_processing import classify_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_in_document(path,documents, allowed_nodes = None):\n",
    "    \"\"\"\n",
    "    Loads documents into the graph database.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path of the document source.\n",
    "        documents (list): A list of document objects to load.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This function iterates over each document in the provided list and loads it into the graph database.\n",
    "    It performs the following steps for each document:\n",
    "    1. Queries the graph database to create a document source if it doesn't exist.\n",
    "    2. Creates summary of document node and classifies document as it relates to AACSB standard\n",
    "    3. Creates a document node for the current document.\n",
    "    4. Creates knowledge graph from LLM extracted info\n",
    "    5. Creates chunk nodes for the text content of the document.\n",
    "    6. Links the chunk nodes to the parent document node.\n",
    "    7. Pauses execution for 1 second to avoid overwhelming the database.\n",
    "    8. Once all documents are processed, links documents within the same source and links source documents to the document source.\n",
    "    9. Links all knowledge graph nodes to parent document node.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        doc_source_result = graphDB.query(doc_source_query, \n",
    "                    params={\n",
    "                        \"docSource\": path,\n",
    "                        \"importDate\": import_date\n",
    "                    })\n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "        for i, document in enumerate(documents):\n",
    "            print(f'Loop Num: {i}')\n",
    "            classification, summary = classify_text(document.page_content)\n",
    "            doc_result = graphDB.query(document_query,\n",
    "                                    params={\n",
    "                                        'docSourceId':doc_source_result[0]['elementId'],\n",
    "                                        'modelSummary': summary,\n",
    "                                        'modelClassification': classification,\n",
    "                                        'page':document.metadata['page'],\n",
    "                                        'source':document.metadata['source'],\n",
    "                                        'idx':i\n",
    "                                    })\n",
    "           \n",
    "            \n",
    "            extract_and_store_graph(document, doc_result[0]['d']['docUUID'], allowed_nodes)\n",
    "\n",
    "            chunk_result = graphDB.query(doc_chunk_query, \n",
    "                                        params ={\n",
    "                                            'parentDocId': doc_result[0]['elementId'],\n",
    "                                            'documentText':document.page_content,\n",
    "                                            'page':document.metadata['page'],\n",
    "                                            'source':document.metadata['source'],\n",
    "                                            'idx':i\n",
    "                                        })\n",
    "            \n",
    "\n",
    "            chunk_to_doc_result = graphDB.query(link_doc_chunk_to_parent_doc,\n",
    "                                            params ={\n",
    "                                                'docParentId':doc_result[0]['elementId']\n",
    "                                            })\n",
    "            time.sleep(1)\n",
    "\n",
    "        doc_link_result = graphDB.query(document_link_query,\n",
    "                                        params ={\n",
    "                                            'docSourceId':doc_source_result[0]['elementId']\n",
    "                                        })\n",
    "\n",
    "        link_docs_to_source_docs_result = graphDB.query(link_to_source_doc, \n",
    "                                                        params ={\n",
    "                                                        'docSourceId':doc_source_result[0]['elementId']\n",
    "                                                        })\n",
    "        link_kg_to_source_docs_result = graphDB.query(link_kg_nodes_to_source_doc)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 459 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "c:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 0.2.0. Use create_structured_output_runnable instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 190. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Input ids are automatically padded from 190 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 449 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 2\n",
      "Loop Num: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 410 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Your max_length is set to 256, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Input ids are automatically padded from 66 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 4\n",
      "Loop Num: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Input ids are automatically padded from 16 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Input ids are automatically padded from 497 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 6\n",
      "Loop Num: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Input ids are automatically padded from 46 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 152. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=76)\n",
      "Input ids are automatically padded from 152 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 9\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Test function\n",
    "#\n",
    "allowed_nodes = [\"Department\", \"Curriculum\", \"Learning Objective\", \"Program\", \"Course\", \"Service\", \"Assessment\", \"Teaching\",\"Business\", \"School of Business\", \"College\", \"University\", \"Staff\",\n",
    "            \"Undergradauate\", \"Graduate\", \"Committee\", \"Mission\", \"Budget\", \"Performance Outcomes\", \"Students\", \"Faculty\", \"News\", \"Strategic Development\", \"Research\", \"Activity\", \"Goals\"]\n",
    "\n",
    "load_in_document(path,documents,allowed_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_link_result = graphDB.query(document_link_query,\n",
    "#                                 params ={\n",
    "#                                     'docSourceId':doc_source_result[0]['elementId']\n",
    "#                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_docs_to_source_docs_result = graphDB.query(link_to_source_doc, \n",
    "#                                                 params ={\n",
    "#                                                    'docSourceId':doc_source_result[0]['elementId']\n",
    "#                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_docs_to_standards_query = \"\"\"\n",
    "MATCH(d: Document), (s: Standard)\n",
    "WHERE d.standardClassification = s.standardNum\n",
    "MERGE (d)-[newRelationship:ASSOCIATED_WITH_STANDARD]->(s)\n",
    "RETURN count(newRelationship) AS num_links_created\n",
    "\"\"\"\n",
    "\n",
    "link_docsource_to_inst_root_query = \"\"\"\n",
    "MATCH (d: Docsource), (r: Root)\n",
    "WHERE r.nodeCat = 'INSTITUTION' AND d.nodeCat = 'INSTITUTION' AND r.nodeType = 'ROOT'\n",
    "MERGE (d)-[newRelationship: PRIMARY_DOCSOURCE_FOR ]->(r)\n",
    "RETURN count(newRelationship) as num_links_created\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'num_links_created': 5}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphDB.query(link_docs_to_standards_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'num_links_created': 1}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphDB.query(link_docsource_to_inst_root_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# INSTITUTION DATA LOAD COMPLETE --> Goto next workbook retriverQA.ipynb\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# #[DONE] move to helper.py\n",
    "\n",
    "# def write_chunks_to_df(chunks):\n",
    "#     chunk_data = []\n",
    "#     for chunk_info in chunks:\n",
    "#         chunk = chunk_info['c']\n",
    "#         chunk_entry = {\n",
    "#             'text': chunk['text'],\n",
    "#             'nodeType': chunk['nodeType'],\n",
    "#             'UUID': chunk['UUID'],\n",
    "#         }\n",
    "#         chunk_data.append(chunk_entry)\n",
    "\n",
    "#     # with open(file_path, 'w') as json_file:\n",
    "#     #     json.dump(chunk_data, json)\n",
    "#     return pd.DataFrame(chunk_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
