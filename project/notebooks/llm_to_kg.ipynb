{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q python-dotenv\n",
    "! pip install -q neo4j\n",
    "! pip install -q langchain\n",
    "! pip install -q langchain-openai\n",
    "! pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv(\"URL\")\n",
    "os.environ[\"NEO4J_USERNAME\"]= os.getenv(\"USERNAME\")\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv(\"PASSWORD2\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDB = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "          \"system\",\n",
    "          f\"\"\"# Knowledge Graph Instructions for GPT-3.5 Turbo\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. You are evaluating information from an academic institution, creating a knowledge graph of concepts related to AACSB accreditation.\n",
    "You should be able to identify learning goals (eg. Written Communication, Oral Communication, Critical Thinking, Ethics, Globalization, Information Technology),\n",
    "along with how and when they are assessed.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "          \"\"\"),\n",
    "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain.chains.openai_functions import (\n",
    "#     create_openai_fn_chain,\n",
    "#     create_structured_output_chain,\n",
    "# )\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "# standard_classification = {1:'strategic planning',\n",
    "#                            2:'physical, virtual and financial resources',\n",
    "#                            3: 'faculty and professional staff resources',\n",
    "#                            4: 'curriculum',\n",
    "#                            5: 'assurance of learning',\n",
    "#                            6:'learner progression',\n",
    "#                            7: 'teaching effectiveness and impact',\n",
    "#                            8: 'impact of scholarship',\n",
    "#                            9: 'engagement and societal impact',\n",
    "#                            0: 'general institution information',\n",
    "#                            }\n",
    "\n",
    "# def get_extraction_chain(\n",
    "#     allowed_nodes: Optional[List[str]] = None,\n",
    "#     allowed_rels: Optional[List[str]] = None\n",
    "#     ):\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [(\n",
    "#           \"system\",\n",
    "#           f\"\"\"# Knowledge Graph Instructions for GPT-3\n",
    "# ## 1. Overview\n",
    "# You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "# - **Nodes** represent entities and concepts. You are evaluating information from an academic institution, creating a knowledge graph of concepts related to AACSB accreditation.\n",
    "# - The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "# ## 2. Labeling Nodes\n",
    "# - **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "#   - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "# - **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "# {'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "# {'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "# ## 3. Handling Numerical Data and Dates\n",
    "# - Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "# - **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "# - **Property Format**: Properties must be in a key-value format. Each concept node should have an attribute key called 'supportsStandard', with an integer value classification. This is the classification value map {standard_classification}\n",
    "\n",
    "# - **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "# - **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "# ## 4. Coreference Resolution\n",
    "# - **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "# If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "# always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "# Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "# ## 5. Strict Compliance\n",
    "# Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "#           \"\"\"),\n",
    "#             (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "#             (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "#         ])\n",
    "#     return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.invoke(document.page_content)['function']\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    graphDB.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q pypdf\n",
    "#! pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import_date = datetime.now()\n",
    "path =r'..\\data\\institution\\sample\\GCSU\\appendix\\gcsu_assurance_of_learning.pdf'\n",
    "\n",
    "# Read the wikipedia article\n",
    "raw_documents = PyPDFLoader(path)\n",
    "pages = raw_documents.load_and_split()\n",
    "\n",
    "\n",
    "# Define chunking strategy\n",
    "#text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000, #changed from 2000\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "# # Only take the first the raw_documents\n",
    "## document chunks\n",
    "documents = text_splitter.split_documents(pages[5:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(documents))\n",
    "print(documents[0].metadata['page'])\n",
    "#print(type(pages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_source_query=\"\"\"\n",
    "MERGE(d:Docsource {source: $docSource})\n",
    "    ON CREATE SET\n",
    "        d.importDate = $importDate,\n",
    "        d.nodeType = 'DOCSOURCE',\n",
    "        d.nodeCat = 'INSTITUTION'\n",
    "RETURN d, elementID(d) as elementId\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# document_query =\"\"\"\n",
    "# MERGE(d:Document {parentDocSourceId: $docSourceId})\n",
    "#    ON CREATE SET\n",
    "#         d.bertSummary = $modelSummary,\n",
    "#         d.standardClassification = $modelClassification,\n",
    "#         d.nodeType = 'DOCUMENT',\n",
    "#         d.nodeCat='INSTITUTION',\n",
    "#         d.pageIdxNum = $page,\n",
    "#         d.source = $source\n",
    "# RETURN d, elementID(d) as elementId\n",
    "# \"\"\"\n",
    "\n",
    "# doc_chunk_query = \"\"\"\n",
    "# MERGE(c:Chunk {parentDocId: $parentDocId})\n",
    "#     ON CREATE SET\n",
    "#         c.text = $documentText,\n",
    "#         c.embedding = NULL,\n",
    "#         c.nodeType = 'DOCUMENTCHUNK',\n",
    "#         c.nodeCat = 'INSTITUTION'\n",
    "# RETURN c, elementID(c) as elementId\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "document_query = \"\"\"\n",
    "CREATE (d:Document {\n",
    "    parentDocSourceId: $docSourceId,\n",
    "    bertSummary: $modelSummary,\n",
    "    standardClassification: $modelClassification,\n",
    "    nodeType: 'DOCUMENT',\n",
    "    nodeCat: 'INSTITUTION',\n",
    "    sourcePageIdxNum: $page,\n",
    "    source: $source,\n",
    "    inputIdx: $idx\n",
    "})\n",
    "RETURN d, elementId(d) as elementId, ID(d) as idNum\n",
    "\"\"\"\n",
    "\n",
    "doc_chunk_query = \"\"\"\n",
    "CREATE (c:Chunk {\n",
    "    UUID: apoc.create.uuid(),\n",
    "    parentDocId: $parentDocId,\n",
    "    text: $documentText,\n",
    "    embedding: null,\n",
    "    nodeType: 'DOCUMENTCHUNK',\n",
    "    nodeCat: 'INSTITUTION',\n",
    "    sourcePageIdxNum: $page,\n",
    "    source: $source,\n",
    "    inputIdx: $idx\n",
    "})\n",
    "RETURN c, elementId(c) as elementId\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_link_query = \"\"\"\n",
    "MATCH (from_same_section:Document)\n",
    "WHERE from_same_section.parentDocSourceId = $docSourceId\n",
    "WITH from_same_section\n",
    "    ORDER BY from_same_section.inputIdx ASC\n",
    "WITH collect(from_same_section) as doc_list\n",
    "    call apoc.nodes.link(\n",
    "        doc_list,\n",
    "        \"NEXT\",\n",
    "        {avoidDuplicates: true}\n",
    "    )\n",
    "    RETURN size(doc_list)\n",
    "\"\"\"\n",
    "\n",
    "link_doc_chunk_to_parent_doc =\"\"\" \n",
    "MATCH (c:Chunk), (d: Document)\n",
    "WHERE c.parentDocId = $docParentId\n",
    "    AND c.sourcePageIdxNum = d.sourcePageIdxNum\n",
    "    AND c.source = d.source\n",
    "    AND c.inputIdx = d.inputIdx\n",
    "MERGE (c)-[newRelationship:PART_OF]->(d)\n",
    "RETURN count(newRelationship) as num_links_created\n",
    "\"\"\"\n",
    "\n",
    "link_to_source_doc = \"\"\" \n",
    "MATCH(d:Document),(s:Docsource)\n",
    "WHERE d.parentDocSourceId = $docSourceId\n",
    "    AND d.source = s.source\n",
    "MERGE (d)-[newRelationship:SOURCE_FROM]->(s)\n",
    "RETURN count(newRelationship) AS num_links_created\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def load_in_document(path,documents):\n",
    "    \"\"\"\n",
    "    Loads documents into the graph database.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path of the document source.\n",
    "        documents (list): A list of document objects to load.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This function iterates over each document in the provided list and loads it into the graph database.\n",
    "    It performs the following steps for each document:\n",
    "    1. Queries the graph database to create a document source if it doesn't exist.\n",
    "    2. Creates a document node for the current document.\n",
    "    3. Creates chunk nodes for the text content of the document.\n",
    "    4. Links the chunk nodes to the parent document node.\n",
    "    5. Pauses execution for 1 second to avoid overwhelming the database.\n",
    "    6. Once all documents are processed, links documents within the same source and links source documents to the document source.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        doc_source_result = graphDB.query(doc_source_query, \n",
    "                    params={\n",
    "                        \"docSource\": path,\n",
    "                        \"importDate\": import_date\n",
    "                    })\n",
    "\n",
    "\n",
    "        for i, document in enumerate(documents):\n",
    "            print(f'Loop Num: {i}')\n",
    "            doc_result = graphDB.query(document_query,\n",
    "                                    params={\n",
    "                                        'docSourceId':doc_source_result[0]['elementId'],\n",
    "                                        'modelSummary': 'placeholder model summary',\n",
    "                                        'modelClassification': '00',\n",
    "                                        'page':document.metadata['page'],\n",
    "                                        'source':document.metadata['source'],\n",
    "                                        'idx':i\n",
    "                                    })\n",
    "            \n",
    "            #\n",
    "            # ADD LLM FUNCTION\n",
    "            #\n",
    "\n",
    "            chunk_result = graphDB.query(doc_chunk_query, \n",
    "                                        params ={\n",
    "                                            'parentDocId': doc_result[0]['elementId'],\n",
    "                                            'documentText':document.page_content,\n",
    "                                            'page':document.metadata['page'],\n",
    "                                            'source':document.metadata['source'],\n",
    "                                            'idx':i\n",
    "                                        })\n",
    "            chunk_to_doc_result = graphDB.query(link_doc_chunk_to_parent_doc,\n",
    "                                            params ={\n",
    "                                                'docParentId':doc_result[0]['elementId']\n",
    "                                            })\n",
    "            time.sleep(1)\n",
    "\n",
    "        doc_link_result = graphDB.query(document_link_query,\n",
    "                                        params ={\n",
    "                                            'docSourceId':doc_source_result[0]['elementId']\n",
    "                                        })\n",
    "\n",
    "        link_docs_to_source_docs_result = graphDB.query(link_to_source_doc, \n",
    "                                                        params ={\n",
    "                                                        'docSourceId':doc_source_result[0]['elementId']\n",
    "                                                        })\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 0\n",
      "Loop Num: 1\n",
      "Loop Num: 2\n",
      "Loop Num: 3\n",
      "Loop Num: 4\n",
      "Loop Num: 5\n",
      "Loop Num: 6\n",
      "Loop Num: 7\n",
      "Loop Num: 8\n",
      "Loop Num: 9\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Test function\n",
    "#\n",
    "load_in_document(path,documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_link_result = graphDB.query(document_link_query,\n",
    "#                                 params ={\n",
    "#                                     'docSourceId':doc_source_result[0]['elementId']\n",
    "#                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_docs_to_source_docs_result = graphDB.query(link_to_source_doc, \n",
    "#                                                 params ={\n",
    "#                                                    'docSourceId':doc_source_result[0]['elementId']\n",
    "#                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# query all chunks\n",
    "#\n",
    "\n",
    "query_all_chunks = \"\"\" \n",
    "MATCH (c:Chunk) WHERE c.embedding IS null\n",
    "RETURN c\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graphDB.query(query_all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "#TODO move to helper.py\n",
    "\n",
    "def write_chunks_to_df(chunks):\n",
    "    chunk_data = []\n",
    "    for chunk_info in chunks:\n",
    "        chunk = chunk_info['c']\n",
    "        chunk_entry = {\n",
    "            'text': chunk['text'],\n",
    "            'nodeType': chunk['nodeType'],\n",
    "            'UUID': chunk['UUID'],\n",
    "        }\n",
    "        chunk_data.append(chunk_entry)\n",
    "\n",
    "    # with open(file_path, 'w') as json_file:\n",
    "    #     json.dump(chunk_data, json)\n",
    "    return pd.DataFrame(chunk_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = r'..\\data\\content_chunks'\n",
    "#write_chunks_to_json(result,out_path)\n",
    "\n",
    "json_dataframe = write_chunks_to_df(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>nodeType</th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The J. Whitney Bunting College of Business 201...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>e165d4cf-ccb2-4d45-91bf-085e3fa3e50b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3. Developing initiatives for student profes...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>258d3cf6-5990-43b3-969d-adc9ec262b05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The J. Whitney Bunting College of Business 201...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>ec725ffb-a27f-4866-b11f-9532bccc2413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>taking for -credit internships. The department...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>b3235929-5054-4e02-a293-7bd7230c791f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the balance sheet.  The results showed that on...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>20e8de4e-bd49-4532-aa6d-5310b4114dd9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time, an interactive classroom tool will be in...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>8d6b206d-1389-4710-b202-1d14bad5449d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The J. Whitney Bunting College of Business 201...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>cad8f788-3b72-4715-a6e4-63ec68c1abbb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BBA Objective 2 .3: Demonstrate basic function...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>5545f05e-9990-4ffe-ba9e-34bc82cab2f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>core business knowledge so the LENB 3135 and M...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>6b6e13c3-63c7-4d04-a2ad-f4e326fb0b1b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>year.  \\n The assessment of student knowledge ...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>33b6f7f0-dee5-4102-8904-660a940cff8b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       nodeType  \\\n",
       "0  The J. Whitney Bunting College of Business 201...  DOCUMENTCHUNK   \n",
       "1  4.3. Developing initiatives for student profes...  DOCUMENTCHUNK   \n",
       "2  The J. Whitney Bunting College of Business 201...  DOCUMENTCHUNK   \n",
       "3  taking for -credit internships. The department...  DOCUMENTCHUNK   \n",
       "4  the balance sheet.  The results showed that on...  DOCUMENTCHUNK   \n",
       "5  time, an interactive classroom tool will be in...  DOCUMENTCHUNK   \n",
       "6  The J. Whitney Bunting College of Business 201...  DOCUMENTCHUNK   \n",
       "7  BBA Objective 2 .3: Demonstrate basic function...  DOCUMENTCHUNK   \n",
       "8  core business knowledge so the LENB 3135 and M...  DOCUMENTCHUNK   \n",
       "9  year.  \\n The assessment of student knowledge ...  DOCUMENTCHUNK   \n",
       "\n",
       "                                   UUID  \n",
       "0  e165d4cf-ccb2-4d45-91bf-085e3fa3e50b  \n",
       "1  258d3cf6-5990-43b3-969d-adc9ec262b05  \n",
       "2  ec725ffb-a27f-4866-b11f-9532bccc2413  \n",
       "3  b3235929-5054-4e02-a293-7bd7230c791f  \n",
       "4  20e8de4e-bd49-4532-aa6d-5310b4114dd9  \n",
       "5  8d6b206d-1389-4710-b202-1d14bad5449d  \n",
       "6  cad8f788-3b72-4715-a6e4-63ec68c1abbb  \n",
       "7  5545f05e-9990-4ffe-ba9e-34bc82cab2f1  \n",
       "8  6b6e13c3-63c7-4d04-a2ad-f4e326fb0b1b  \n",
       "9  33b6f7f0-dee5-4102-8904-660a940cff8b  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Documentation: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "#\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client =OpenAI()\n",
    "\n",
    "def get_embedding(text, model = \"text-embedding-3-small\"):\n",
    "\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dataframe['vector'] = json_dataframe['text'].apply(lambda x:get_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>nodeType</th>\n",
       "      <th>UUID</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The J. Whitney Bunting College of Business 201...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>e165d4cf-ccb2-4d45-91bf-085e3fa3e50b</td>\n",
       "      <td>[-0.002387269865721464, 0.030054964125156403, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3. Developing initiatives for student profes...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>258d3cf6-5990-43b3-969d-adc9ec262b05</td>\n",
       "      <td>[-0.022018861025571823, 0.014942693524062634, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The J. Whitney Bunting College of Business 201...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>ec725ffb-a27f-4866-b11f-9532bccc2413</td>\n",
       "      <td>[-0.021682489663362503, 0.03815522789955139, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>taking for -credit internships. The department...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>b3235929-5054-4e02-a293-7bd7230c791f</td>\n",
       "      <td>[-0.02500932849943638, 0.023677470162510872, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the balance sheet.  The results showed that on...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>20e8de4e-bd49-4532-aa6d-5310b4114dd9</td>\n",
       "      <td>[0.02478775382041931, 0.049934349954128265, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time, an interactive classroom tool will be in...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>8d6b206d-1389-4710-b202-1d14bad5449d</td>\n",
       "      <td>[-0.043007466942071915, 0.0022174373734742403,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The J. Whitney Bunting College of Business 201...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>cad8f788-3b72-4715-a6e4-63ec68c1abbb</td>\n",
       "      <td>[0.018656976521015167, 0.058810919523239136, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BBA Objective 2 .3: Demonstrate basic function...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>5545f05e-9990-4ffe-ba9e-34bc82cab2f1</td>\n",
       "      <td>[-0.02717341110110283, 0.022137731313705444, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>core business knowledge so the LENB 3135 and M...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>6b6e13c3-63c7-4d04-a2ad-f4e326fb0b1b</td>\n",
       "      <td>[-0.011547845788300037, 0.03375094756484032, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>year.  \\n The assessment of student knowledge ...</td>\n",
       "      <td>DOCUMENTCHUNK</td>\n",
       "      <td>33b6f7f0-dee5-4102-8904-660a940cff8b</td>\n",
       "      <td>[-0.020592421293258667, 0.0009046140476129949,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       nodeType  \\\n",
       "0  The J. Whitney Bunting College of Business 201...  DOCUMENTCHUNK   \n",
       "1  4.3. Developing initiatives for student profes...  DOCUMENTCHUNK   \n",
       "2  The J. Whitney Bunting College of Business 201...  DOCUMENTCHUNK   \n",
       "3  taking for -credit internships. The department...  DOCUMENTCHUNK   \n",
       "4  the balance sheet.  The results showed that on...  DOCUMENTCHUNK   \n",
       "5  time, an interactive classroom tool will be in...  DOCUMENTCHUNK   \n",
       "6  The J. Whitney Bunting College of Business 201...  DOCUMENTCHUNK   \n",
       "7  BBA Objective 2 .3: Demonstrate basic function...  DOCUMENTCHUNK   \n",
       "8  core business knowledge so the LENB 3135 and M...  DOCUMENTCHUNK   \n",
       "9  year.  \\n The assessment of student knowledge ...  DOCUMENTCHUNK   \n",
       "\n",
       "                                   UUID  \\\n",
       "0  e165d4cf-ccb2-4d45-91bf-085e3fa3e50b   \n",
       "1  258d3cf6-5990-43b3-969d-adc9ec262b05   \n",
       "2  ec725ffb-a27f-4866-b11f-9532bccc2413   \n",
       "3  b3235929-5054-4e02-a293-7bd7230c791f   \n",
       "4  20e8de4e-bd49-4532-aa6d-5310b4114dd9   \n",
       "5  8d6b206d-1389-4710-b202-1d14bad5449d   \n",
       "6  cad8f788-3b72-4715-a6e4-63ec68c1abbb   \n",
       "7  5545f05e-9990-4ffe-ba9e-34bc82cab2f1   \n",
       "8  6b6e13c3-63c7-4d04-a2ad-f4e326fb0b1b   \n",
       "9  33b6f7f0-dee5-4102-8904-660a940cff8b   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.002387269865721464, 0.030054964125156403, ...  \n",
       "1  [-0.022018861025571823, 0.014942693524062634, ...  \n",
       "2  [-0.021682489663362503, 0.03815522789955139, 0...  \n",
       "3  [-0.02500932849943638, 0.023677470162510872, 0...  \n",
       "4  [0.02478775382041931, 0.049934349954128265, 0....  \n",
       "5  [-0.043007466942071915, 0.0022174373734742403,...  \n",
       "6  [0.018656976521015167, 0.058810919523239136, 0...  \n",
       "7  [-0.02717341110110283, 0.022137731313705444, 0...  \n",
       "8  [-0.011547845788300037, 0.03375094756484032, 0...  \n",
       "9  [-0.020592421293258667, 0.0009046140476129949,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_to_chunk_query = \"\"\" \n",
    "MATCH (c:Chunk {UUID: $UUID})\n",
    "SET c.embedding = $vector\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in json_dataframe.iterrows():\n",
    "\n",
    "    graphDB.query(vector_to_chunk_query, \n",
    "                params ={\n",
    "                    'UUID':row['UUID'],\n",
    "                    'vector':row['vector']\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Vector Index and Embeddings (may move to a different notebook)\n",
    "# https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/\n",
    "\n",
    "\n",
    "#Create Vector Index\n",
    "\n",
    "vector_index_query=\"\"\" \n",
    "CALL db.index.vector.createNodeIndex(\n",
    "  'accreditation-index',\n",
    "  'Chunk',\n",
    "  'embedding',\n",
    "   1536,\n",
    "  'cosine'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "graphDB.query(vector_index_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 3,\n",
       "  'name': 'accreditation-index',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'VECTOR',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['embedding'],\n",
       "  'indexProvider': 'vector-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': None,\n",
       "  'readCount': 0},\n",
       " {'id': 1,\n",
       "  'name': 'index_343aff4e',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2024, 4, 15, 8, 8, 55, 947000000, tzinfo=<UTC>),\n",
       "  'readCount': 35},\n",
       " {'id': 2,\n",
       "  'name': 'index_f7700477',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'RELATIONSHIP',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': None,\n",
       "  'readCount': 0}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphDB.query(\"SHOW INDEXES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphDB.query(\"SHOW PROCEDURES YIELD *\")\n",
    "\n",
    "semantic_index_query = \"\"\"\n",
    "\n",
    "CALL db.index.vector.queryNodes('accreditation-index', 2, $inputVector)\n",
    "YIELD node AS responseNode, score\n",
    "\n",
    "RETURN responseNode.text, score \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"what are the marketing courses\"\n",
    "query_vector = get_embedding(query_text)\n",
    "\n",
    "query_result = graphDB.query(semantic_index_query, \n",
    "                                params={\n",
    "                                    \"inputVector\":query_vector\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'responseNode.text': 'BBA Objective 2 .3: Demonstrate basic functional abilities across core business subjects  \\nBecause this goal covers a large area of knowledge, two courses taken by all business majors w as \\nassessed in addition to using the ETS exam as an overall assessment.   \\n LENB 3135 was used to assess how well students could discuss the laws that relate to contracts, \\nincluding the UCC. In 14- 15, the fall semester students met or exceed the target at an 80% rate while the \\nspring semester seemed to be an anomaly as only 62% met or exceeded the target. The same assessment was used in 15- 16. For fall semester, 74% met or exceeded expectations, and in spring 16, \\n72% met or exceeded expectations . \\n \\nMKTG 3161 was used to assess how well students identify key marketing concepts and apply them to \\nreal-world business problems. In 13- 14, 68% met the target, but another 16% only missed by 1 question. \\nThis assessment was  continued with continued emphasis on repetition of the key concepts for the 14- 15 \\nyear.  In 14- 15, 81% of students met or exceeded the target. This was higher than the goal of 80%. More \\nemphasis was given on missed questions in past semesters in an effort to further improve student \\nlearning and capability to apply the concepts to real -world business problems. The 80% goal remains a \\nstretch for non -marketing majors which make up the majority of the class. The same assessment was \\nused again in 15- 16, but the post questions were included on the final exam rather than a separate post -\\ntest. This increased the value of the questions to the students and 93% of students taking both the pre \\nand post questions scored the targeted 80% or higher on th e post questions.  \\n \\nThe Undergraduate Curriculum Committee decided that only the ETS will be used in the future to assess \\ncore business knowledge so the LENB 3135 and MKTG 3161 assessments will not be repeated next \\nyear.',\n",
       "  'score': 0.7234116196632385},\n",
       " {'responseNode.text': 'core business knowledge so the LENB 3135 and MKTG 3161 assessments will not be repeated next \\nyear.  \\n The assessment of student knowledge of core business concepts ( including accounting, finance, creation',\n",
       "  'score': 0.7138975262641907}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Documentation References: \n",
    "# https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/#:~:text=As%20of%20Neo4j%205.13%2C%20you,IEEE%20754%20single%20precision%20values\n",
    "# https://neo4j.com/labs/genai-ecosystem/apoc-genai/\n",
    "#\n",
    "\n",
    "\n",
    "# chunk_embedding_query = \"\"\"\n",
    "#     MATCH (c:Chunk) WHERE c.embedding IS null\n",
    "#     WITH c, apoc.ml.openai.embedding(\n",
    "#       c.text, \n",
    "#       \"OpenAI\", \n",
    "#       {\n",
    "#         token: $openAiApiKey \n",
    "#         }) AS vector\n",
    "#     CALL db.create.setVectorProperty(c, \"embedding\", vector)\n",
    "#     YIELD node\n",
    "#     \"\"\"\n",
    "\n",
    "# chunk_embedding_query = \"\"\"\n",
    "#     MATCH (c:Chunk) WHERE c.embedding IS null\n",
    "#     WITH c, apoc.ml.openai.embedding([c.text], $openAiApiKey, {}) AS result\n",
    "#     CALL {\n",
    "#         WITH c, result\n",
    "#         CALL db.create.setVectorProperty(c, \"embedding\", result.embedding[0])\n",
    "#         YIELD node\n",
    "#         RETURN node\n",
    "#     }\n",
    "#     RETURN c\n",
    "#     \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphDB.query(chunk_embedding_query, \n",
    "#               params={\n",
    "#                   'openAiApiKey': os.environ[\"OPENAI_API_KEY\"]\n",
    "#               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve_chunks_for_embed_query = \"\"\" \n",
    "# MATCH(c:Chunk) WHERE c.embedding is null RETURN count(c) AS numNodes\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphDB.query(retrieve_chunks_for_embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    try:\n",
    "        extract_and_store_graph(d)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for document {i}: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDB.query(\"MATCH (n) DETACH DELETE n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which node labels should be extracted by the LLM\n",
    "allowed_nodes = [\"Person\", \"Company\", \"Location\", \"Event\", \"Movie\", \"Service\", \"Award\"]\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    extract_and_store_graph(d, allowed_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "\n",
    "graphDB.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graphDB,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_chain.invoke({\"query\": \"How many learning objectives are assessed\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_chain.invoke({\"query\": \"Which student learning goals were identified\"}) # does not know\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_chain.invoke({\"query\": \"What are the descriptions of the Learning goal\"}) ## does not know , see still Learning goal example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
