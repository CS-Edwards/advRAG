{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q python-dotenv\n",
    "! pip install -q neo4j\n",
    "! pip install -q langchain\n",
    "! pip install -q langchain-openai\n",
    "! pip install -q tiktoken\n",
    "! pip install -q torch\n",
    "! pip install -q transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv(\"URL\")\n",
    "os.environ[\"NEO4J_USERNAME\"]= os.getenv(\"USERNAME\")\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv(\"PASSWORD2\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Neo4jGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m graphDB \u001b[38;5;241m=\u001b[39m \u001b[43mNeo4jGraph\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Neo4jGraph' is not defined"
     ]
    }
   ],
   "source": [
    "graphDB = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "    parentDocUUID: str,\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "          \"system\",\n",
    "          f\"\"\"# Knowledge Graph Instructions for GPT-3.5 Turbo\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. You are evaluating information from an academic institution, creating a knowledge graph of concepts related to AACSB accreditation.\n",
    "You should be able to identify learning goals (eg. Written Communication, Oral Communication, Critical Thinking, Ethics, Globalization, Information Technology),\n",
    "along with how and when they are assessed.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- ** Provided REQUIRED Property**: Each node must have a the property key \"parentDocUUID\", this is the value {parentDocUUID}. DO NOT CREATE a VALUE only use the provided value.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "          \"\"\"),\n",
    "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    parentDocUUID: str,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(parentDocUUID, nodes, rels)\n",
    "    data = extract_chain.invoke(document.page_content)['function']\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    graphDB.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import_date = datetime.now()\n",
    "path =r'..\\data\\institution\\sample\\GCSU\\appendix\\gcsu_assurance_of_learning.pdf'\n",
    "\n",
    "# Read the wikipedia article\n",
    "raw_documents = PyPDFLoader(path)\n",
    "pages = raw_documents.load_and_split()\n",
    "\n",
    "\n",
    "# Define chunking strategy\n",
    "#text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000, #changed from 2000\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "# # Only take the first the raw_documents\n",
    "## document chunks\n",
    "documents = text_splitter.split_documents(pages[5:35])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(documents))\n",
    "#print(documents[0].metadata['page'])\n",
    "#print(type(pages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_source_query=\"\"\"\n",
    "MERGE(d:Docsource {source: $docSource})\n",
    "    ON CREATE SET\n",
    "        d.importDate = $importDate,\n",
    "        d.nodeType = 'DOCSOURCE',\n",
    "        d.nodeCat = 'INSTITUTION'\n",
    "RETURN d, elementID(d) as elementId\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# document_query =\"\"\"\n",
    "# MERGE(d:Document {parentDocSourceId: $docSourceId})\n",
    "#    ON CREATE SET\n",
    "#         d.bertSummary = $modelSummary,\n",
    "#         d.standardClassification = $modelClassification,\n",
    "#         d.nodeType = 'DOCUMENT',\n",
    "#         d.nodeCat='INSTITUTION',\n",
    "#         d.pageIdxNum = $page,\n",
    "#         d.source = $source\n",
    "# RETURN d, elementID(d) as elementId\n",
    "# \"\"\"\n",
    "\n",
    "# doc_chunk_query = \"\"\"\n",
    "# MERGE(c:Chunk {parentDocId: $parentDocId})\n",
    "#     ON CREATE SET\n",
    "#         c.text = $documentText,\n",
    "#         c.embedding = NULL,\n",
    "#         c.nodeType = 'DOCUMENTCHUNK',\n",
    "#         c.nodeCat = 'INSTITUTION'\n",
    "# RETURN c, elementID(c) as elementId\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "document_query = \"\"\"\n",
    "CREATE (d:Document {\n",
    "    parentDocSourceId: $docSourceId,\n",
    "    docUUID: apoc.create.uuid(),\n",
    "    modSummary: $modelSummary,\n",
    "    standardClassification: $modelClassification,\n",
    "    nodeType: 'DOCUMENT',\n",
    "    nodeCat: 'INSTITUTION',\n",
    "    sourcePageIdxNum: $page,\n",
    "    source: $source,\n",
    "    inputIdx: $idx\n",
    "})\n",
    "RETURN d, d.docUUID as elementId, ID(d) as idNum\n",
    "\"\"\"\n",
    "\n",
    "doc_chunk_query = \"\"\"\n",
    "CREATE (c:Chunk {\n",
    "    UUID: apoc.create.uuid(),\n",
    "    parentDocId: $parentDocId,\n",
    "    text: $documentText,\n",
    "    embedding: null,\n",
    "    nodeType: 'DOCUMENTCHUNK',\n",
    "    nodeCat: 'INSTITUTION',\n",
    "    sourcePageIdxNum: $page,\n",
    "    source: $source,\n",
    "    inputIdx: $idx\n",
    "})\n",
    "RETURN c, elementId(c) as elementId\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_link_query = \"\"\"\n",
    "MATCH (from_same_section:Document)\n",
    "WHERE from_same_section.parentDocSourceId = $docSourceId\n",
    "WITH from_same_section\n",
    "    ORDER BY from_same_section.inputIdx ASC\n",
    "WITH collect(from_same_section) as doc_list\n",
    "    call apoc.nodes.link(\n",
    "        doc_list,\n",
    "        \"NEXT\",\n",
    "        {avoidDuplicates: true}\n",
    "    )\n",
    "    RETURN size(doc_list)\n",
    "\"\"\"\n",
    "\n",
    "link_doc_chunk_to_parent_doc =\"\"\" \n",
    "MATCH (c:Chunk), (d: Document)\n",
    "WHERE c.parentDocId = $docParentId\n",
    "    AND c.sourcePageIdxNum = d.sourcePageIdxNum\n",
    "    AND c.source = d.source\n",
    "    AND c.inputIdx = d.inputIdx\n",
    "MERGE (c)-[newRelationship:PART_OF]->(d)\n",
    "RETURN count(newRelationship) as num_links_created\n",
    "\"\"\"\n",
    "\n",
    "link_to_source_doc = \"\"\" \n",
    "MATCH(d:Document),(s:Docsource)\n",
    "WHERE d.parentDocSourceId = $docSourceId\n",
    "    AND d.source = s.source\n",
    "MERGE (d)-[newRelationship:SOURCE_FROM]->(s)\n",
    "RETURN count(newRelationship) AS num_links_created\n",
    "\"\"\"\n",
    "\n",
    "link_kg_nodes_to_source_doc = \"\"\" \n",
    "MATCH (n), (d:Document) \n",
    "WHERE d.docUUID = n.parentdocuuid \n",
    "MERGE (n)-[newRelationship: CONTENT_FROM_DOC]->(d) \n",
    "RETURN count(newRelationship)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../utils')  \n",
    "from input_processing import classify_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_in_document(path,documents, allowed_nodes = None):\n",
    "    \"\"\"\n",
    "    Loads documents into the graph database.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path of the document source.\n",
    "        documents (list): A list of document objects to load.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This function iterates over each document in the provided list and loads it into the graph database.\n",
    "    It performs the following steps for each document:\n",
    "    1. Queries the graph database to create a document source if it doesn't exist.\n",
    "    2. Creates summary of document node and classifies document as it relates to AACSB standard\n",
    "    3. Creates a document node for the current document.\n",
    "    4. Creates knowledge graph from LLM extracted info\n",
    "    5. Creates chunk nodes for the text content of the document.\n",
    "    6. Links the chunk nodes to the parent document node.\n",
    "    7. Pauses execution for 1 second to avoid overwhelming the database.\n",
    "    8. Once all documents are processed, links documents within the same source and links source documents to the document source.\n",
    "    9. Links all knowledge graph nodes to parent document node.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        doc_source_result = graphDB.query(doc_source_query, \n",
    "                    params={\n",
    "                        \"docSource\": path,\n",
    "                        \"importDate\": import_date\n",
    "                    })\n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "        for i, document in enumerate(documents):\n",
    "            print(f'Loop Num: {i}')\n",
    "            classification, summary = classify_text(document.page_content)\n",
    "            doc_result = graphDB.query(document_query,\n",
    "                                    params={\n",
    "                                        'docSourceId':doc_source_result[0]['elementId'],\n",
    "                                        'modelSummary': summary,\n",
    "                                        'modelClassification': classification,\n",
    "                                        'page':document.metadata['page'],\n",
    "                                        'source':document.metadata['source'],\n",
    "                                        'idx':i\n",
    "                                    })\n",
    "           \n",
    "            \n",
    "            extract_and_store_graph(document, doc_result[0]['d']['docUUID'], allowed_nodes)\n",
    "\n",
    "            chunk_result = graphDB.query(doc_chunk_query, \n",
    "                                        params ={\n",
    "                                            'parentDocId': doc_result[0]['elementId'],\n",
    "                                            'documentText':document.page_content,\n",
    "                                            'page':document.metadata['page'],\n",
    "                                            'source':document.metadata['source'],\n",
    "                                            'idx':i\n",
    "                                        })\n",
    "            \n",
    "\n",
    "            chunk_to_doc_result = graphDB.query(link_doc_chunk_to_parent_doc,\n",
    "                                            params ={\n",
    "                                                'docParentId':doc_result[0]['elementId']\n",
    "                                            })\n",
    "            time.sleep(1)\n",
    "\n",
    "        doc_link_result = graphDB.query(document_link_query,\n",
    "                                        params ={\n",
    "                                            'docSourceId':doc_source_result[0]['elementId']\n",
    "                                        })\n",
    "\n",
    "        link_docs_to_source_docs_result = graphDB.query(link_to_source_doc, \n",
    "                                                        params ={\n",
    "                                                        'docSourceId':doc_source_result[0]['elementId']\n",
    "                                                        })\n",
    "        link_kg_to_source_docs_result = graphDB.query(link_kg_nodes_to_source_doc)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 459 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "c:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 0.2.0. Use create_structured_output_runnable instead.\n",
      "  warn_deprecated(\n",
      "Your max_length is set to 256, but your input_length is only 190. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Input ids are automatically padded from 190 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 449 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 410 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Input ids are automatically padded from 66 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Input ids are automatically padded from 16 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 497 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 6\n",
      "Loop Num: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Input ids are automatically padded from 46 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 152. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=76)\n",
      "Input ids are automatically padded from 152 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 503 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 237. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=118)\n",
      "Input ids are automatically padded from 237 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 405 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 364 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 453 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 381 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 427 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 390 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 415 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 431 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 40. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "Input ids are automatically padded from 40 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Input ids are automatically padded from 74 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 501 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 452 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 400 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 24\n",
      "Loop Num: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Input ids are automatically padded from 51 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 53. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Input ids are automatically padded from 53 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 461 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 433 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 422 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 419 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 93. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Input ids are automatically padded from 93 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Input ids are automatically padded from 78 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 392 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 436 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Input ids are automatically padded from 71 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Input ids are automatically padded from 82 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 407 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 424 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 39\n",
      "Loop Num: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 393 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 84. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      "Input ids are automatically padded from 84 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 84. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 428 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 442 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 294 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 464 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 325 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 423 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Num: 49\n",
      "Loop Num: 50\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for _OutputFormatter\noutput -> rels\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Test function\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      4\u001b[0m allowed_nodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurriculum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning Objective\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProgram\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCourse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssessment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeaching\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBusiness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchool of Business\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollege\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStaff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndergradauate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraduate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommittee\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMission\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBudget\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformance Outcomes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFaculty\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStrategic Development\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResearch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoals\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[43mload_in_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mallowed_nodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 78\u001b[0m, in \u001b[0;36mload_in_document\u001b[1;34m(path, documents, allowed_nodes)\u001b[0m\n\u001b[0;32m     76\u001b[0m     link_kg_to_source_docs_result \u001b[38;5;241m=\u001b[39m graphDB\u001b[38;5;241m.\u001b[39mquery(link_kg_nodes_to_source_doc)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[14], line 49\u001b[0m, in \u001b[0;36mload_in_document\u001b[1;34m(path, documents, allowed_nodes)\u001b[0m\n\u001b[0;32m     37\u001b[0m classification, summary \u001b[38;5;241m=\u001b[39m classify_text(document\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[0;32m     38\u001b[0m doc_result \u001b[38;5;241m=\u001b[39m graphDB\u001b[38;5;241m.\u001b[39mquery(document_query,\n\u001b[0;32m     39\u001b[0m                         params\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     40\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocSourceId\u001b[39m\u001b[38;5;124m'\u001b[39m:doc_source_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melementId\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m:i\n\u001b[0;32m     46\u001b[0m                         })\n\u001b[1;32m---> 49\u001b[0m \u001b[43mextract_and_store_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocUUID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m chunk_result \u001b[38;5;241m=\u001b[39m graphDB\u001b[38;5;241m.\u001b[39mquery(doc_chunk_query, \n\u001b[0;32m     52\u001b[0m                             params \u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     53\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparentDocId\u001b[39m\u001b[38;5;124m'\u001b[39m: doc_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melementId\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m:i\n\u001b[0;32m     58\u001b[0m                             })\n\u001b[0;32m     61\u001b[0m chunk_to_doc_result \u001b[38;5;241m=\u001b[39m graphDB\u001b[38;5;241m.\u001b[39mquery(link_doc_chunk_to_parent_doc,\n\u001b[0;32m     62\u001b[0m                                 params \u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     63\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocParentId\u001b[39m\u001b[38;5;124m'\u001b[39m:doc_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melementId\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     64\u001b[0m                                 })\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mextract_and_store_graph\u001b[1;34m(document, parentDocUUID, nodes, rels)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_and_store_graph\u001b[39m(\n\u001b[0;32m      2\u001b[0m     document: Document,\n\u001b[0;32m      3\u001b[0m     parentDocUUID: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m      4\u001b[0m     nodes:Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     rels:Optional[List[\u001b[38;5;28mstr\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Extract graph data using OpenAI functions\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     extract_chain \u001b[38;5;241m=\u001b[39m get_extraction_chain(parentDocUUID, nodes, rels)\n\u001b[1;32m----> 8\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Construct a graph document\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     graph_document \u001b[38;5;241m=\u001b[39m GraphDocument(\n\u001b[0;32m     11\u001b[0m       nodes \u001b[38;5;241m=\u001b[39m [map_to_base_node(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mnodes],\n\u001b[0;32m     12\u001b[0m       relationships \u001b[38;5;241m=\u001b[39m [map_to_base_relationship(rel) \u001b[38;5;28;01mfor\u001b[39;00m rel \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mrels],\n\u001b[0;32m     13\u001b[0m       source \u001b[38;5;241m=\u001b[39m document\n\u001b[0;32m     14\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\llm.py:104\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\llm.py:258\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[1;34m(self, llm_result)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         {\n\u001b[0;32m    261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse_result(generation),\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[0;32m    263\u001b[0m         }\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[0;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\llm.py:261\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         {\n\u001b[1;32m--> 261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[0;32m    263\u001b[0m         }\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[0;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_core\\output_parsers\\openai_functions.py:219\u001b[0m, in \u001b[0;36mPydanticAttrOutputFunctionsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 219\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattr_name)\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_core\\output_parsers\\openai_functions.py:204\u001b[0m, in \u001b[0;36mPydanticOutputFunctionsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    202\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_only:\n\u001b[1;32m--> 204\u001b[0m     pydantic_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_result\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m _result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\pydantic\\v1\\main.py:549\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[1;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationError([ErrorWrapper(e, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\pydantic\\v1\\main.py:526\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m    524\u001b[0m         exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected dict not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobj)\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for _OutputFormatter\noutput -> rels\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Test function\n",
    "#\n",
    "allowed_nodes = [\"Department\", \"Curriculum\", \"Learning Objective\", \"Program\", \"Course\", \"Service\", \"Assessment\", \"Teaching\",\"Business\", \"School of Business\", \"College\", \"University\", \"Staff\",\n",
    "            \"Undergradauate\", \"Graduate\", \"Committee\", \"Mission\", \"Budget\", \"Performance Outcomes\", \"Students\", \"Faculty\", \"News\", \"Strategic Development\", \"Research\", \"Activity\", \"Goals\"]\n",
    "\n",
    "load_in_document(path,documents,allowed_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_link_result = graphDB.query(document_link_query,\n",
    "#                                 params ={\n",
    "#                                     'docSourceId':doc_source_result[0]['elementId']\n",
    "#                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_docs_to_source_docs_result = graphDB.query(link_to_source_doc, \n",
    "#                                                 params ={\n",
    "#                                                    'docSourceId':doc_source_result[0]['elementId']\n",
    "#                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_docs_to_standards_query = \"\"\"\n",
    "MATCH(d: Document), (s: Standard)\n",
    "WHERE d.standardClassification = s.standardNum\n",
    "MERGE (d)-[newRelationship:ASSOCIATED_WITH_STANDARD]->(s)\n",
    "RETURN count(newRelationship) AS num_links_created\n",
    "\"\"\"\n",
    "\n",
    "link_docsource_to_inst_root_query = \"\"\"\n",
    "MATCH (d: Docsource), (r: Root)\n",
    "WHERE r.nodeCat = 'INSTITUTION' AND d.nodeCat = 'INSTITUTION' AND r.nodeType = 'ROOT'\n",
    "MERGE (d)-[newRelationship: PRIMARY_DOCSOURCE_FOR ]->(r)\n",
    "RETURN count(newRelationship) as num_links_created\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgraphDB\u001b[49m\u001b[38;5;241m.\u001b[39mquery(link_docs_to_standards_query)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graphDB' is not defined"
     ]
    }
   ],
   "source": [
    "graphDB.query(link_docs_to_standards_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'num_links_created': 1}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphDB.query(link_docsource_to_inst_root_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# INSTITUTION DATA LOAD COMPLETE --> Goto next workbook retriverQA.ipynb\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# #[DONE] move to helper.py\n",
    "\n",
    "# def write_chunks_to_df(chunks):\n",
    "#     chunk_data = []\n",
    "#     for chunk_info in chunks:\n",
    "#         chunk = chunk_info['c']\n",
    "#         chunk_entry = {\n",
    "#             'text': chunk['text'],\n",
    "#             'nodeType': chunk['nodeType'],\n",
    "#             'UUID': chunk['UUID'],\n",
    "#         }\n",
    "#         chunk_data.append(chunk_entry)\n",
    "\n",
    "#     # with open(file_path, 'w') as json_file:\n",
    "#     #     json.dump(chunk_data, json)\n",
    "#     return pd.DataFrame(chunk_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
