{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q python-dotenv\n",
    "! pip install -q neo4j\n",
    "! pip install -q langchain\n",
    "! pip install -q langchain-openai\n",
    "! pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv(\"URL\")\n",
    "os.environ[\"NEO4J_USERNAME\"]= os.getenv(\"USERNAME\")\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv(\"PASSWORD2\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import sys\n",
    "sys.path.append('../utils')  \n",
    "from helper import write_chunks_to_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDB = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphDB.query(\"SHOW INDEXES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initialize Semantic Vector Index\n",
    "#\n",
    "# DOCS: https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/\n",
    "#\n",
    "\n",
    "vector_index_query=\"\"\" \n",
    "CALL db.index.vector.createNodeIndex(\n",
    "  'accreditation_index',\n",
    "  'Chunk',\n",
    "  'embedding',\n",
    "   1536,\n",
    "  'cosine'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "graphDB.query(vector_index_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDB.query(\"SHOW INDEXES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Query All Chunks without embedding\n",
    "#\n",
    "\n",
    "all_chunks_query = \"\"\" \n",
    "MATCH (c:Chunk) \n",
    "WHERE c.embedding IS null OR c.embedding = 0\n",
    "RETURN c\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# attach embedding to chunk.embedding\n",
    "vector_to_chunk_query = \"\"\" \n",
    "MATCH (c:Chunk {UUID: $UUID})\n",
    "SET c.embedding = $vector\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graphDB.query(all_chunks_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate Vectors for Chunk.text and update Chunk.embedding\n",
    "#\n",
    "\n",
    "chunk_dataframe = write_chunks_to_df(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk_dataframe.head()\n",
    "chunk_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Chunk embedding function\n",
    "# \n",
    "# DOCS: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "#\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client =OpenAI()\n",
    "\n",
    "#MODEL =  \"text-embedding-3-small\"\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "def get_embedding(text, model = MODEL):\n",
    "\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_dataframe['vector'] = chunk_dataframe['text'].apply(lambda x:get_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in chunk_dataframe.iterrows():\n",
    "\n",
    "    graphDB.query(vector_to_chunk_query, \n",
    "                params ={\n",
    "                    'UUID':row['UUID'],\n",
    "                    'vector':row['vector']\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_index_query = \"\"\"\n",
    "\n",
    "CALL db.index.vector.queryNodes('accreditation_index', 2, $inputVector)\n",
    "YIELD node AS responseNode, score\n",
    "\n",
    "RETURN responseNode.text, score \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_text = \"what are aacsb standards\"\n",
    "# query_vector = get_embedding(query_text)\n",
    "# query_result = graphDB.query(semantic_index_query, \n",
    "#                                 params={\n",
    "#                                     \"inputVector\":query_vector\n",
    "#                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Standard QA\n",
    "#\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "VECTOR_INDEX_NAME = 'accreditiation-index'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY='embedding'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "log_dir = os.path.abspath('../logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "#Info logger\n",
    "info_logger = logging.getLogger('info_logger')\n",
    "info_logger.setLevel(logging.INFO)\n",
    "info_handler = logging.FileHandler(os.path.join(log_dir, 'info.log'))\n",
    "info_logger.addHandler(info_handler)\n",
    "\n",
    "#Error logger\n",
    "error_logger = logging.getLogger('error_logger')\n",
    "error_logger.setLevel(logging.ERROR)\n",
    "error_handler = logging.FileHandler(os.path.join(log_dir, 'error.log'))\n",
    "error_logger.addHandler(error_handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sucessful test\n",
    "#info_logger.info(\"testing info logger\")\n",
    "#error_logger.error(\"testing error logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Sourced from documentation and modified: https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/\n",
    "#\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "\n",
    "Perfect Syntax: Your queries must be in the correct Cypher syntax, at all costs you should avoide\n",
    "'CypherSyntaxError' and 'ValueError' resulting from your query ie:\n",
    "ValueError: Generated Cypher Statement is not valid\n",
    "code: Neo.ClientError.Statement.SyntaxError message: Invalid input 'objective': expected \")\", \"WHERE\", or a parameter (line 1, column 19 (offset: 18))\n",
    "\"MATCH (n:Learning objective) RETURN n.name\"\n",
    "\n",
    "Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
    "\n",
    "## AACSB STANDARD EXAMPLE\n",
    "# Which standards deal with staff resources?\n",
    "MATCH (n)\n",
    "WHERE n.nodeCat = 'AACSB' AND (n.text CONTAINS 'staff' AND n.text CONTAINS 'resources' OR n.text CONTAINS 'staff resources')\n",
    "RETURN n\n",
    "\n",
    "# How is standard 2 documented\n",
    "MATCH (d:Documentation) WHERE d.parentStandardNum = 2 RETURN d.text\n",
    "\n",
    "## INSTITUTION EXAMPLE -- schema may vary from example, reference schema\n",
    "# Which learning objectives did undergradute and graduate program evaluate\n",
    "MATCH (p:Program)-[]->(l:`Learning objective`)\n",
    "WHERE (p.name CONTAINS 'undergraduate' OR p.name CONTAINS 'graduate') \n",
    "RETURN l.name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# RExample of returning nodes:\n",
    "# MATCH (p:Program)-[]->(l:`Learning objective`)\n",
    "# WHERE (p.name CONTAINS 'undergraduate' OR p.name CONTAINS 'graduate') \n",
    "# RETURN l,p\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDB.refresh_schema()\n",
    "schema = graphDB.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 2)\n",
    "pp.pprint(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "graphDB.refresh_schema()\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graphDB,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose= True,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #cypher_chain.invoke({\"query\": \"How many learning objectives are assessed\"})\n",
    "# #result = cypher_chain.invoke({\"query\": \"Which student learning goals were identified\"}) # does not know\n",
    "# result = cypher_chain.invoke({\"query\": \"What is standard 5 about\"}) # does not know #works well\n",
    "\n",
    "# #cypher_chain.invoke({\"query\": \"What are the descriptions of the Learning goal\"}) ## does not know , see still Learning goal example\n",
    "# #cypher_chain.invoke({\"query\": \"Tell me about AACSB standards\"}) ## does not know , see still Learning goal example\n",
    "# print(result['result'])\n",
    "# print(result['intermediate_steps'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box LLM genreate cypher queries are not producing any results, OR are producing errors.Provide some examples to the mode an oppopriate queries based on the schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain  = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  LangChain Docs: https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever/\n",
    "#     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "\n",
    "def extract_list(llm_result):\n",
    "    content = llm_result.content\n",
    "    #return content.split(\"\\\\n\") # bug?\n",
    "    result = content.split(\"[SEP]\")\n",
    "    if not isinstance(result, list):\n",
    "            raise TypeError(f\"Expected a list, but received {type(content)} instead.\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# NOT USED IN PIPELINE\n",
    "def generate_multi_question(question):\n",
    "\n",
    "    system_prompt = f\"\"\" \n",
    "\n",
    "    # Instruction\n",
    "    You are an AI language model assistant. Your task is to generate three \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by new line. \n",
    "    \n",
    "    # Format Rules\n",
    "    DO NOT NUMBER THE LIST\n",
    "    Original question: {question}\n",
    "    \"\"\" \n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "    result = llm.invoke(system_prompt)\n",
    "    query_list = extract_list(result)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "    return {'original_query': question, 'generated_query_list': query_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_multi_question_aacsb(question: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    For queries relating to the AACSB standards, generates list three different versions\n",
    "    of the user's input query. Used downstream for multiquery retrieval.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's input query.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, any]: A dictionary containing the original user question and the generated list of alternative \n",
    "        questions. Keys include 'original_query' for the original question and 'generated_query_list' for the list \n",
    "        of alternative questions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    system_prompt = f\"\"\" \n",
    "\n",
    "    # Instruction\n",
    "    You are an AI language model assistant. Your task is to generate three \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search, by providing precise questions.\n",
    "    Provide these alternative questions separated by \"[SEP]\" TOKEN.\n",
    "\n",
    "    # List Delimiter\n",
    "    Each question in the list must be seperated by the \"[SEP]\" TOKEN\n",
    "\n",
    "    # Important Context: AACSB\n",
    "    The questions you generate are directly related to extracting useful information on the \n",
    "    AACSB accrediation standards. The questions you generate will be used as vector database index\n",
    "    queries that contain information on:\n",
    "\n",
    "    - formal AACSB descriptions\n",
    "    - documentation that supports each standard \n",
    "    - basis for evaluation of the standards \n",
    "    - relevent definitions of terms used in the standard descriptions.\n",
    "\n",
    "    The AACSB Website provides the following summary of their work:\n",
    "    AACSB accreditation is known, worldwide, as the longest-standing, most recognized form of \n",
    "    specialized accreditation that an institution and its business programs can earn. \n",
    "    Accreditation is a voluntary, nongovernmental process that includes a rigorous external review \n",
    "    of a school's mission, faculty qualifications, curricula, and ability to provide the highest-quality programs.\n",
    "    \n",
    "    # Format Rules\n",
    "    DO NOT NUMBER THE LIST\n",
    "    DO NOT ANSWER THE QUESTION\n",
    "    DELIMITER USING \"[SEP]\" token\n",
    "    Original question: {question}\n",
    "    \"\"\" \n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    result = llm.invoke(system_prompt)\n",
    "    query_list = extract_list(result)  \n",
    "\n",
    "    log_data = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "        'level': 'INFO',\n",
    "        'function_name': 'generate_multi_question_institution',\n",
    "        'original_query': question,\n",
    "        'generated_query_list': query_list\n",
    "    }\n",
    "    info_logger.info(json.dumps(log_data))\n",
    "\n",
    "    return {'original_query': question, 'generated_query_list': query_list, 'cat':'AACSB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def generate_multi_question_institution(question: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    For school specific, or academic institution specific, queries. Generates list three different versions\n",
    "    of the user's input query. Used downstream for multiquery retrieval.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's input query.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, any]: A dictionary containing the original user question and the generated list of alternative \n",
    "        questions. Keys include 'original_query' for the original question and 'generated_query_list' for the list \n",
    "        of alternative questions.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    system_prompt = f\"\"\" \n",
    "\n",
    "    # Instruction\n",
    "    You are an AI language model assistant. Your task is to generate three \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search, by providing precise questions.\n",
    "    Provide these alternative questions separated by \"[SEP]\" TOKEN.\n",
    "\n",
    "    # List Delimiter\n",
    "    Each question in the list must be seperated by the \"[SEP]\" TOKEN\n",
    "\n",
    "    # Important Context: Academic Institution\n",
    "    The questions you generate are directly related to extracting useful information about a School\n",
    "    of Business.  The questions you generate will be used as vector database index\n",
    "    queries that contain information on:\n",
    "\n",
    "    - Strategic Plan, Mission and Fiscal Resources\n",
    "    - Academic Departments in the School of Business inluding not limited to : Accounting, Marketing, Management, Finance, Entreprenuership\n",
    "    - Student Services and Student Organizations \n",
    "    - Program Goals, Learning Objectives and Curriculum Assessment\n",
    "    - Continuous Improvement\n",
    "\n",
    "\n",
    "    # Format Rules\n",
    "    DO NOT NUMBER THE LIST\n",
    "    DO NOT ANSWER THE QUESTION\n",
    "    DELIMITER USING \"[SEP]\" token\n",
    "    Original question: {question}\n",
    "    \"\"\" \n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    result = llm.invoke(system_prompt)\n",
    "    query_list = extract_list(result)  \n",
    "\n",
    "    log_data = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "    'level': 'INFO',\n",
    "    'function_name': 'generate_multi_question_institution',\n",
    "    'original_query': question,\n",
    "    'generated_query_list': query_list\n",
    "        }\n",
    "    info_logger.info(json.dumps(log_data))                                                                      \n",
    "\n",
    "    return {'original_query': question, 'generated_query_list': query_list, 'cat':'INSTITUTION'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def generate_sub_questions_hybrid(question: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    For queries relating to both AACSB accreditation AND Academic Institution (School) information .\n",
    "    Generates two distinct sub questions based on the user's input query. Used downstream for multiquery retrieval.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's input query.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, any]: A dictionary containing the original user question and the generated list of alternative \n",
    "        questions. Keys include 'original_query' for the original question and 'generated_query_list' for the list \n",
    "        of alternative questions.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    system_prompt = f\"\"\" \n",
    "\n",
    "    # Instruction\n",
    "    You are an AI language model assistant. Your task is to evalute the users input query and \n",
    "    divide the query into two and ONLY TWO sub questions. The first sub questions should address the portion\n",
    "    of the user query that relates to the AACSB Standards, the second subquestion should relate the institution\n",
    "    specific portion of the query. Your overall objective is to break down the complex user query into the \n",
    "    two distinct sub questions.  Provide these alternative questions separated by \"[SEP]\" TOKEN.\n",
    "\n",
    "    # List Delimiter\n",
    "    Each question in the list must be seperated by the \"[SEP]\" TOKEN\n",
    "\n",
    "    # Important Context: Sub \n",
    "    \n",
    "    ## 1. AACSB Standard sub question:\n",
    "    AACSB sub question may related to accreditation content such as:\n",
    "\n",
    "    - formal AACSB descriptions\n",
    "    - documentation that supports each standard \n",
    "    - basis for evaluation of the standards \n",
    "    - relevent definitions of terms used in the standard descriptions.\n",
    "\n",
    "    The AACSB Website provides the following summary of their work:\n",
    "    AACSB accreditation is known, worldwide, as the longest-standing, most recognized form of \n",
    "    specialized accreditation that an institution and its business programs can earn. \n",
    "    Accreditation is a voluntary, nongovernmental process that includes a rigorous external review \n",
    "    of a school's mission, faculty qualifications, curricula, and ability to provide the highest-quality programs.\n",
    "\n",
    "\n",
    "    ## 2. Academic Instiution sub question:\n",
    "    Academic Instiution sub question may relate to School of Business content such as:\n",
    "\n",
    "    - Strategic Plan, Mission and Fiscal Resources\n",
    "    - Academic Departments in the School of Business inluding not limited to : Accounting, Marketing, Management, Finance, Entreprenuership\n",
    "    - Student Services and Student Organizations \n",
    "    - Program Goals, Learning Objectives and Curriculum Assessment\n",
    "    - Continuous Improvement\n",
    "\n",
    "\n",
    "    # Format Rules\n",
    "    DO NOT NUMBER THE LIST\n",
    "    DO NOT ANSWER THE QUESTION\n",
    "    DELIMITER USING \"[SEP]\" token\n",
    "    Original question: {question}\n",
    "    \"\"\" \n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    result = llm.invoke(system_prompt)\n",
    "    query_list = extract_list(result)  \n",
    "\n",
    "    log_data = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "        'level': 'INFO',\n",
    "        'function_name': 'generate_multi_question_institution',\n",
    "        'original_query': question,\n",
    "        'generated_query_list': query_list\n",
    "    }\n",
    "    info_logger.info(json.dumps(log_data))\n",
    "\n",
    "    return {'original_query': question, 'generated_query_list': query_list, 'cat':'HYBRID'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_query = \"how should I prepare for extended travel\"\n",
    "\n",
    "# result = generate_multi_question(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in result['generated_query_list']:\n",
    "#     print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_query = \"Our accountind department recently  updated the curriculum to include carbon footprint, does this reflect the sustainability standard \"\n",
    "# test_hybrid_query = generate_sub_questions_hybrid(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_hybrid_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "def optimize_query_function_route(user_query:str)->str:\n",
    "    \"\"\"\n",
    "    Run a conversation with OpenAI's language model, providing the user query and available functions to the model.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input query.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the language model.\n",
    "\n",
    "    Step 1: Send the conversation and available functions to the model.\n",
    "        - Each function is described with its name, description, and parameters.\n",
    "        - Three functions are available:\n",
    "            1. generate_multi_question_aacsb: Generates three different versions of the user's input query\n",
    "               related to AACSB standards.\n",
    "            2. generate_multi_question_institution: Generates three different versions of the user's input query\n",
    "               for school-specific or academic institution-specific queries.\n",
    "            3. generate_sub_questions_hybrid: Generates two distinct sub-questions based on the user's input query,\n",
    "               relating to both AACSB accreditation and academic institution (school) information.\n",
    "\n",
    "    Returns the response from the language model, specifically the first choice message from the available choices.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"generate_multi_question_aacsb\",\n",
    "                \"description\": \"For queries relating to the AACSB standards, generates list three different versions of the user's input query. Used downstream for multiquery retrieval.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Input user query\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"question\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"generate_multi_question_institution\",\n",
    "                \"description\": \"For school specific, or academic institution specific, queries. Generates list three different versions of the user's input query. Used downstream for multiquery retrieval.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Input user query\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"question\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"generate_sub_questions_hybrid\",\n",
    "                \"description\": \"For queries relating to both AACSB accreditation AND Academic Institution (School) information. Generates two distinct sub questions based on the user's input query. Used downstream for multiquery retrieval.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Input user query\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"question\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default\n",
    "    )\n",
    "    return response.choices[0].message\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_query = \"AACSB standards on sustainability\"\n",
    "# result = optimize_query_function_route(input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# function_name =  result.tool_calls[0].function.name\n",
    "# function_args = json.loads(result.tool_calls[0].function.arguments)['question']\n",
    "# print(f\"{function_name} , {function_args}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function = globals()[function_name]\n",
    "# print(function(function_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def execute_route_function(conv_result: any)->any:\n",
    "    \"\"\"\n",
    "    Execute a route function based on the provided conversation result.\n",
    "\n",
    "    Args:\n",
    "        conv_result (any): The conversation result containing information about the function to execute.\n",
    "\n",
    "    Returns:\n",
    "        any: The result of executing the route function.\n",
    "\n",
    "    Extracts the name and arguments of the route function from the conversation result and then\n",
    "    dynamically executes the function using the extracted information.\n",
    "\n",
    "    Note:\n",
    "        - The function_name and function_args are extracted from the tool_calls attribute of the conversation result.\n",
    "        - The function_name is used to retrieve the actual function from the global namespace.\n",
    "        - The function_args are passed as arguments to the retrieved function.\n",
    "    \"\"\"\n",
    "\n",
    "    function_name =  conv_result.tool_calls[0].function.name\n",
    "    function_args = json.loads(conv_result.tool_calls[0].function.arguments)['question']\n",
    "    function = globals()[function_name]\n",
    "    return function(function_args)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_routing_pipeline(user_query):\n",
    "    \"\"\"\n",
    "    Run a conversation with OpenAI's language model using the provided user query and then execute the route function.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input query.\n",
    "\n",
    "    Returns:\n",
    "        dict: The result of executing the route function.\n",
    "\n",
    "    This function serves as a pipeline for querying and routing based on the user's input.\n",
    "    It first runs a conversation with OpenAI's language model using the provided user query.\n",
    "    The result of the conversation is then passed to the execute_route_function to determine and execute the appropriate route function.\n",
    "    The result of executing the route function is returned as a dictionary.\n",
    "    \"\"\"\n",
    "    result = optimize_query_function_route(user_query)\n",
    "    return execute_route_function(result) #dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_query_in = \"AACSB standards on sustainability\"\n",
    "# pipeline_result = query_routing_pipeline(test_query_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query_dict, cypher_chain=cypher_chain):\n",
    "\n",
    "    \n",
    "    context_data = []\n",
    "\n",
    "    if query_dict['cat'] != 'HYBRID':\n",
    "        # multiqueries -- case A\n",
    "        # single cypher query for original query\n",
    "        for q in query_dict['generated_query_list']:\n",
    "            query_vector = get_embedding(q)\n",
    "            query_result = graphDB.query(semantic_index_query, \n",
    "                                params={\n",
    "                                    \"inputVector\":query_vector\n",
    "                                })\n",
    "            query_text = [text['responseNode.text'] for text in query_result]\n",
    "\n",
    "            log_data = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "            'level': 'INFO',\n",
    "            'function_name': 'retriever() - MULTI',\n",
    "            'original_query': query_dict['original_query'],\n",
    "            'current_query': q,\n",
    "            'semantic_query_result': query_text,\n",
    "\n",
    "                }\n",
    "            info_logger.info(json.dumps(log_data))\n",
    "\n",
    "\n",
    "\n",
    "            context_data.extend(query_text)\n",
    "        \n",
    "        #cypher_result = cypher_chain.invoke({\"query\": query_dict['original_query']})\n",
    "\n",
    "        try:\n",
    "            cypher_result = cypher_chain.invoke({\"query\": query_dict['original_query']})\n",
    "      \n",
    "            log_data = {\n",
    "                        'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "                        'level': 'INFO',\n",
    "                        'function_name': 'retriever() - MULTI CYPHER',\n",
    "                        'original_query': query_dict['original_query'],\n",
    "                        'cypher_query': cypher_result['result'],\n",
    "                        'cypher_steps': cypher_result['intermediate_steps']\n",
    "\n",
    "                            }\n",
    "            info_logger.info(json.dumps(log_data))\n",
    "                \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "        except Exception as e:\n",
    "                log_data = {\n",
    "                        'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "                        'level': 'ERROR',\n",
    "                        'function_name': 'retriever()- MULTI CYPHER',\n",
    "                        'original_query': query_dict['original_query'],\n",
    "                        'cypher_query_1': cypher_result,\n",
    "                        'ErrorException': e\n",
    "                        }\n",
    "                error_logger.error(json.dumps(log_data))\n",
    "                cypher_result = \"\" #set empty for context string down steam\n",
    "\n",
    "        if \"I don't know the answer\" not in cypher_result['result']:\n",
    "            context_data.append(cypher_result['result'])\n",
    "            \n",
    "\n",
    "    \n",
    "    else:\n",
    "        # subqueries -- case B\n",
    "        # cypher query for each subquery\n",
    "        for q in query_dict['generated_query_list']:\n",
    "            query_vector = get_embedding(q)\n",
    "            query_result = graphDB.query(semantic_index_query, \n",
    "                                params={\n",
    "                                    \"inputVector\":query_vector\n",
    "                                })\n",
    "            query_text = [text['responseNode.text'] for text in query_result]\n",
    "\n",
    "            log_data = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "            'level': 'INFO',\n",
    "            'function_name': 'retriever() - SUB',\n",
    "            'original_query': query_dict['original_query'],\n",
    "            'current_query':q,\n",
    "            'semantic_query_result': query_text,\n",
    "\n",
    "            }\n",
    "            info_logger.info(json.dumps(log_data))\n",
    "            context_data.extend(query_text)\n",
    "\n",
    "            try:\n",
    "                cypher_result = cypher_chain.invoke({\"query\": q})\n",
    "                log_data = {\n",
    "                        'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "                        'level': 'INFO',\n",
    "                        'function_name': 'retriever() -SUB CYPHER',\n",
    "                        'original_query': query_dict['original_query'],\n",
    "                        'cypher_query': cypher_result['result'],\n",
    "                        'cypher_steps': cypher_result['intermediate_steps']\n",
    "\n",
    "                            }\n",
    "                info_logger.info(json.dumps(log_data))\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_data = {\n",
    "                        'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "                        'level': 'ERROR',\n",
    "                        'function_name': 'retriever() - SUB-CYPHER',\n",
    "                        'original_query': query_dict['original_query'],\n",
    "                        'cypher_query_2': cypher_result\n",
    "                            }\n",
    "                error_logger.error(json.dumps(log_data))\n",
    "                cypher_result = \"\" #set empty for context string down steam\n",
    "\n",
    "            if \"I don't know the answer\" not in cypher_result['result']:\n",
    "                context_data.append(cypher_result['result'])\n",
    "                \n",
    "    # Final Context Str\n",
    "    context_str = ', '.join(str(item) for item in context_data)\n",
    "\n",
    "    # Log context_str \n",
    "  \n",
    "    log_data = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "    'level': 'INFO',\n",
    "    'function_name': 'retriever()',\n",
    "    'original_query': query_dict['original_query'],\n",
    "    'context_string': context_str\n",
    "    }\n",
    "\n",
    "    info_logger.info(json.dumps(log_data))\n",
    "\n",
    "\n",
    "\n",
    "    #if not hybrid\n",
    "    ### loop through multiqueries\n",
    "    #### call embedding function on each and retrieve data from db\n",
    "    #### append to  context data\n",
    "    ### call cypher query, append results to cyher data\n",
    "\n",
    "    #else:\n",
    "    ## loop through sub queries\n",
    "    ### call embedding and retrieve append resutls to context data\n",
    "    ### call cypher, append results to context data\n",
    "\n",
    "    return context_str #return context data as a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ret_result = retriever(pipeline_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(original_query, context_string, model = \"gpt-3.5-turbo-16k\"):\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    # Instruction:\n",
    "    You are an AI assistant generating a thorough and thoughtful response to a user query.\n",
    "    You are to generate a response that answers their query solely based on the context provided below\n",
    "    do not use any other outside information. The grounding context information has been retrieved from\n",
    "    a database which is the absolute knowledge source. \n",
    "\n",
    "    # User Query:\n",
    "    {original_query}\n",
    "\n",
    "    # Grounding Context:\n",
    "    {context_string}\n",
    "\n",
    "    # Output Style\n",
    "    Your tone should be professional. And your response should be detailed, as this information\n",
    "    will be used to generate reports. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not context_string:\n",
    "        log_data = {\n",
    "                    'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "                    'level': 'ERROR',\n",
    "                    'function_name': 'generator()',\n",
    "                    'original_query': original_query,\n",
    "                    'context_string': 0\n",
    "\n",
    "                        }\n",
    "        error_logger.error(json.dumps(log_data))\n",
    "        return \"Generator Issue: context string is empty, unable to generate response, please rephrase your question and try again...\"\n",
    "\n",
    "    llm = ChatOpenAI(model=model, temperature=0)\n",
    "    result = llm.invoke(system_prompt)\n",
    "    content = result.content\n",
    "\n",
    "    # Log Response\n",
    "    log_data = {\n",
    "                    'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "                    'level': 'INFO',\n",
    "                    'function_name': 'generator()',\n",
    "                    'original_query': original_query,\n",
    "                    'context_string': context_string,\n",
    "                    'generatedResponse': content\n",
    "\n",
    "                        }\n",
    "    info_logger.info(json.dumps(log_data))\n",
    "\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = [\n",
    "    \"AACSB Standards on sustainability\", #aacsb standards\n",
    "    \"Our accounting department updated curriculum to include environmental impact in business risk evaluation, does this reflect the sustanability standard\", #hybrid subquestion()\n",
    "    \"Standard 5 specifies a systematic process for assurance of learning. What do peer review teams usually expect in determining whether this standard is met?\",\n",
    "    \"What are intellectual contributions\",\n",
    "    \"Must faculty members publish in order to be qualified as scholarly academic, practice academic, or scholarly practitioner?\", #FAQ\n",
    "    \"Who is the dean of my school\",\n",
    "    \"What are the management courses\",\n",
    "    \"Which leaning objectives are assessed in the undergradute business program \",\n",
    "    \"How is standard 8 defined?\",\n",
    "    \"What are MACC program students expected to demonstrate\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen = generator(test_query_in, ret_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advRagPipeline(query):\n",
    "    try:\n",
    "        query_pipeline_result = query_routing_pipeline(query)\n",
    "        context_str = retriever(query_pipeline_result)\n",
    "        output = generator(query,context_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (s:Standard {title: \"AACSB Standards on sustainability\"}) RETURN s\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (c:Curriculum)-[:INCLUDES_COURSE]->(co:Course {name: 'Environmental Impact in Business Risk Evaluation'}), (s:Standard {title: 'Sustainability Standard'}) RETURN c, co, s\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_out = []\n",
    "#\n",
    "#\n",
    "# Stalls 30mins +\n",
    "\n",
    "# for q in test_query:\n",
    "#     output = advRagPipeline(q)\n",
    "#     test_out.append(output)\n",
    "\n",
    "\n",
    "def run(query):\n",
    "    output = advRagPipeline(query)\n",
    "    test_out.append(output)\n",
    "\n",
    "run(test_query[0])\n",
    "run(test_query[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (s:Standard {standardNum: 5})<-[:BASIS_OF]-(b:Basis) RETURN b.text\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'b.text': '5.1 assurance learning processes school identifies learning competencies business degree program well appropriate direct indirect measures systematically assessed demonstrate learning competencies achieved across degree programs . competencies derive consonant school mission strategies expected outcomes reported degree level opposed major level . competencies curriculum management processes reflect currency knowledge expectations stakeholders including limited organizations employing graduates alumni learners university community policymakers . competencies largely achieved . competencies achieved school provides evidence actions taken remediate deficiencies . direct indirect measures employed school expected include types measures across entire portfolio assessment degree programs . proportion direct versus indirect measures degree program determined school consistent mission strategic initiatives . acceptable programs assessed direct measures programs may assessed indirect measures . school provides rationale determining programs measured direct measures programs measured indirect measures . results regular direct indirect assessment lead curricular process improvements . school employs systematic aol process includes meaningful broad faculty participation . programs launched since last review robust aol plan place including timeline gathering analyzing data . depending long program offered data may may yet gathered . program offered five years would expected gathered sufficient data demonstrate systematic effective process program however program launched one two years normal peer review visit may yet gathered sufficient data demonstrate systematic effective process . standards intend case newly launched degree program schools given sufficient time establish systematic assessment process adequately demonstrates student learning case robust assessment plan paramount importance . 5.2 degree equivalency expectations learner effort outcomes degree credentials equivalent terms depth rigor regardless delivery mode location . competency based education cbe credit awarded school normally equivalent quality assured via direct assessment learners . cbe credit reflect small percentage total academic program . 5.3 stackable microlearning credentials credentials certificates minors badges lead degree program evaluated degree program level . 5.4 non degree executive education non degree executive education normally reviewed overall quality continuous improvement customer client satisfaction programs generate greater five percent school annual resources .'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run(test_query[2])\n",
    "run(test_query[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mThis question cannot be answered as the provided schema does not contain any information or relationship types related to a \"dean\" or a \"school\".\u001b[0m\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cypher_result' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:246\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:313\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_disabled_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:181\u001b[0m, in \u001b[0;36mResult._run\u001b[1;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:301\u001b[0m, in \u001b[0;36mResult._attach\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:850\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    847\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    848\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    849\u001b[0m )\n\u001b[1;32m--> 850\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[1;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'This': expected\r\n  \"ALTER\"\r\n  \"CALL\"\r\n  \"CREATE\"\r\n  \"DEALLOCATE\"\r\n  \"DELETE\"\r\n  \"DENY\"\r\n  \"DETACH\"\r\n  \"DROP\"\r\n  \"DRYRUN\"\r\n  \"ENABLE\"\r\n  \"FOREACH\"\r\n  \"GRANT\"\r\n  \"LOAD\"\r\n  \"MATCH\"\r\n  \"MERGE\"\r\n  \"OPTIONAL\"\r\n  \"REALLOCATE\"\r\n  \"REMOVE\"\r\n  \"RENAME\"\r\n  \"RETURN\"\r\n  \"REVOKE\"\r\n  \"SET\"\r\n  \"SHOW\"\r\n  \"START\"\r\n  \"STOP\"\r\n  \"TERMINATE\"\r\n  \"UNWIND\"\r\n  \"USE\"\r\n  \"USING\"\r\n  \"WITH\" (line 1, column 1 (offset: 0))\r\n\"This question cannot be answered as the provided schema does not contain any information or relationship types related to a \"dean\" or a \"school\".\"\r\n ^}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 35\u001b[0m, in \u001b[0;36mretriever\u001b[1;34m(query_dict, cypher_chain)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     cypher_result \u001b[38;5;241m=\u001b[39m \u001b[43mcypher_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal_query\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     log_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     38\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     39\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m                     }\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    158\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\graph_qa\\cypher.py:268\u001b[0m, in \u001b[0;36mGraphCypherQAChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_cypher:\n\u001b[1;32m--> 268\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_cypher\u001b[49m\u001b[43m)\u001b[49m[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k]\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:252\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CypherSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Cypher Statement is not valid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'This': expected\r\n  \"ALTER\"\r\n  \"CALL\"\r\n  \"CREATE\"\r\n  \"DEALLOCATE\"\r\n  \"DELETE\"\r\n  \"DENY\"\r\n  \"DETACH\"\r\n  \"DROP\"\r\n  \"DRYRUN\"\r\n  \"ENABLE\"\r\n  \"FOREACH\"\r\n  \"GRANT\"\r\n  \"LOAD\"\r\n  \"MATCH\"\r\n  \"MERGE\"\r\n  \"OPTIONAL\"\r\n  \"REALLOCATE\"\r\n  \"REMOVE\"\r\n  \"RENAME\"\r\n  \"RETURN\"\r\n  \"REVOKE\"\r\n  \"SET\"\r\n  \"SHOW\"\r\n  \"START\"\r\n  \"STOP\"\r\n  \"TERMINATE\"\r\n  \"UNWIND\"\r\n  \"USE\"\r\n  \"USING\"\r\n  \"WITH\" (line 1, column 1 (offset: 0))\r\n\"This question cannot be answered as the provided schema does not contain any information or relationship types related to a \"dean\" or a \"school\".\"\r\n ^}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m run(test_query[\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(query):\n\u001b[1;32m---> 11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43madvRagPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     test_out\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m, in \u001b[0;36madvRagPipeline\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m generator(query,context_str)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36madvRagPipeline\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     query_pipeline_result \u001b[38;5;241m=\u001b[39m query_routing_pipeline(query)\n\u001b[1;32m----> 4\u001b[0m     context_str \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_pipeline_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m generator(query,context_str)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[20], line 58\u001b[0m, in \u001b[0;36mretriever\u001b[1;34m(query_dict, cypher_chain)\u001b[0m\n\u001b[0;32m     46\u001b[0m     info_logger\u001b[38;5;241m.\u001b[39minfo(json\u001b[38;5;241m.\u001b[39mdumps(log_data))\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     53\u001b[0m         log_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     54\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     55\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     56\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretriever()- MULTI CYPHER\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     57\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_query\u001b[39m\u001b[38;5;124m'\u001b[39m: query_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_query\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 58\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcypher_query_1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcypher_result\u001b[49m,\n\u001b[0;32m     59\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrorException\u001b[39m\u001b[38;5;124m'\u001b[39m: e\n\u001b[0;32m     60\u001b[0m                 }\n\u001b[0;32m     61\u001b[0m         error_logger\u001b[38;5;241m.\u001b[39merror(json\u001b[38;5;241m.\u001b[39mdumps(log_data))\n\u001b[0;32m     62\u001b[0m         cypher_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#set empty for context string down steam\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'cypher_result' referenced before assignment"
     ]
    }
   ],
   "source": [
    "run(test_query[4])\n",
    "run(test_query[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run(test_query[6])\n",
    "run(test_query[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (s:Standard {standardNum: 8})<-[:TERM_DEFINITIONS_FOR]-(d:Definitions) RETURN d.text\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'d.text': 'society context refers external stakeholders relevance business school given mission . examples include nonprofit private sector organizations business government community groups broader social economic business physical environments . external stakeholders broader environments may local regional national international scale . thought leadership evidenced business school recognized highly respected authority area areas expertise thus sought relevant stakeholders . aligned school mission stakeholders include learners business academics government nonprofits non governmental organizations broader society . predatory journals publishers defined entities prioritize self interest expense scholarship characterized false misleading information deviation best editorial publication practices lack transparency use aggressive indiscriminate solicitation practices . 18 definition adopted predatory journals definition defence . nature 2019 december 11 . retrieved https www.nature.com articles d41586 019 03759 intellectual contributions original works intended advance theory practice teaching business . intellectual contributions may potential address issues importance broader society . contributions scholarly sense based generally accepted academic research principles disseminated appropriate audiences . school portfolio intellectual contributions may fall following categories basic discovery scholarship directed toward increasing knowledge base development theory . applied integrative application scholarship draws basic research uses accumulated theories knowledge methods techniques solve real world problems issues associated practice . teaching learning scholarship explores theory methods teaching advances new understandings insights content methods impact learning behavior . addition categorization intellectual contributions within portfolio basic applied teaching learning related schools characterize intellectual contributions according level peer expert review occurred intellectual contributions appearing portfolio . purposes peer reviewed intellectual contributions subject scrutiny evaluation others recognized subject matter expertise field normally similar competence producing outputs . component separated three parts peer reviewed journal articles scholarly publications submitted critique evaluation one academics expertise discipline methodology subject matter . publications law reviews may included category . peer editorial reviewed intellectual contributions include forms quality assurance either peers subject matter experts recognized particular practical academic expertise field . examples include papers submitted academic conference undergo peer review selected conference presentation publication conference proceedings . articles published practitioner industry publications included category sufficiently reviewed subject matter experts . intellectual contributions journal articles papers sufficiently influential public policy government industry included category sufficiently reviewed subject matter experts . simply writing paper output consulting work render appropriate category . review validation subject matter experts determining factor whether intellectual contribution belongs category third category intellectual contributions . intellectual contributions include outputs validated peers recognized subject matter experts . contributions include wide variety outputs presentations academic professional meetings research workshops led invited talks etc .'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Program {name: 'MACC'})-[:HASLEARNINGOBJECTIVE]->(lo:Learning objective)\n",
      "RETURN lo.name, lo.description\u001b[0m\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cypher_result' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:246\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:313\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_disabled_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:181\u001b[0m, in \u001b[0;36mResult._run\u001b[1;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:301\u001b[0m, in \u001b[0;36mResult._attach\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:850\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    847\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    848\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    849\u001b[0m )\n\u001b[1;32m--> 850\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[1;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'objective': expected \")\", \"WHERE\", \"{\" or a parameter (line 1, column 72 (offset: 71))\r\n\"MATCH (p:Program {name: 'MACC'})-[:HASLEARNINGOBJECTIVE]->(lo:Learning objective)\"\r\n                                                                        ^}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 35\u001b[0m, in \u001b[0;36mretriever\u001b[1;34m(query_dict, cypher_chain)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     cypher_result \u001b[38;5;241m=\u001b[39m \u001b[43mcypher_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal_query\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     log_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     38\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     39\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m                     }\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    158\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain\\chains\\graph_qa\\cypher.py:268\u001b[0m, in \u001b[0;36mGraphCypherQAChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_cypher:\n\u001b[1;32m--> 268\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_cypher\u001b[49m\u001b[43m)\u001b[49m[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k]\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Candace Edwards\\capstone\\advRAG\\myenv\\lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:252\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CypherSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Cypher Statement is not valid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'objective': expected \")\", \"WHERE\", \"{\" or a parameter (line 1, column 72 (offset: 71))\r\n\"MATCH (p:Program {name: 'MACC'})-[:HASLEARNINGOBJECTIVE]->(lo:Learning objective)\"\r\n                                                                        ^}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m run(test_query[\u001b[38;5;241m8\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(query):\n\u001b[1;32m---> 11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43madvRagPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     test_out\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m, in \u001b[0;36madvRagPipeline\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m generator(query,context_str)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36madvRagPipeline\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     query_pipeline_result \u001b[38;5;241m=\u001b[39m query_routing_pipeline(query)\n\u001b[1;32m----> 4\u001b[0m     context_str \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_pipeline_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m generator(query,context_str)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[20], line 58\u001b[0m, in \u001b[0;36mretriever\u001b[1;34m(query_dict, cypher_chain)\u001b[0m\n\u001b[0;32m     46\u001b[0m     info_logger\u001b[38;5;241m.\u001b[39minfo(json\u001b[38;5;241m.\u001b[39mdumps(log_data))\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     53\u001b[0m         log_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     54\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     55\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     56\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretriever()- MULTI CYPHER\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     57\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_query\u001b[39m\u001b[38;5;124m'\u001b[39m: query_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_query\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 58\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcypher_query_1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcypher_result\u001b[49m,\n\u001b[0;32m     59\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrorException\u001b[39m\u001b[38;5;124m'\u001b[39m: e\n\u001b[0;32m     60\u001b[0m                 }\n\u001b[0;32m     61\u001b[0m         error_logger\u001b[38;5;241m.\u001b[39merror(json\u001b[38;5;241m.\u001b[39mdumps(log_data))\n\u001b[0;32m     62\u001b[0m         cypher_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#set empty for context string down steam\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'cypher_result' referenced before assignment"
     ]
    }
   ],
   "source": [
    "run(test_query[8])\n",
    "run(test_query[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "======================================================\n",
      " 0: AACSB Standards on sustainability \n",
      "\n",
      " 0: The AACSB (Association to Advance Collegiate Schools of Business) has established standards on sustainability that schools must adhere to in order to attain and maintain participating supporting status. These standards are periodically reviewed to ensure they reflect a focus on continuous improvement.\n",
      "\n",
      "One of the criteria for attaining participating supporting status is the sufficiency of faculty members. The school must have faculty members who are actively participating and supporting the mission of the school. The criteria for faculty sufficiency should be documented and consistent with the mission of the school. The school may adapt guidance based on their specific situation in developing and implementing criteria that indicate they are meeting the spirit and intent of the standard.\n",
      "\n",
      "In addition to faculty sufficiency, the criteria for attaining participating supporting status also address the depth and breadth of activities expected within a typical AACSB accreditation review cycle. These activities should be maintained to demonstrate ongoing commitment to sustainability.\n",
      "\n",
      "Furthermore, participating faculty members are expected to deliver at least 75 percent of the school's teaching globally, across the entire accredited unit. Additionally, faculty members should deliver at least 60 percent of the teaching within their discipline, regardless of whether the school offers degrees, majors, concentrations, or other programs in that discipline.\n",
      "\n",
      "It is important to note that these criteria are periodically reviewed to ensure they reflect the evolving expectations and best practices in sustainability education.\n",
      "\n",
      "Overall, the AACSB standards on sustainability encompass a range of criteria and activities that schools must meet to attain and maintain participating supporting status. By adhering to these standards, schools can demonstrate their commitment to sustainability and continuous improvement in business education. \n",
      "\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      " 1: Our accounting department updated curriculum to include environmental impact in business risk evaluation, does this reflect the sustanability standard \n",
      "\n",
      " 1: Based on the information provided, it appears that your accounting department has updated its curriculum to include environmental impact in business risk evaluation. This is a positive step towards reflecting sustainability standards in your organization's practices.\n",
      "\n",
      "The curriculum update aligns with the criteria outlined in the school's faculty sufficiency standards. These standards require faculty members to participate and support the consistent mission of the school. By incorporating environmental impact in business risk evaluation, your accounting department is demonstrating its commitment to addressing societal impact related outcomes, as outlined in standards 1, 4, 8, and 9.\n",
      "\n",
      "Furthermore, the criteria for faculty sufficiency emphasize the importance of attaining participating and supporting status by engaging in activities that align with the school's mission. The periodic review of these criteria ensures that the depth and breadth of activities expected within a typical AACSB accreditation review cycle are maintained.\n",
      "\n",
      "It is worth noting that the school may choose to use the United Nations Sustainable Development Goals as a label to indicate its chosen focus area. This can provide a clear indication of the school's commitment to sustainability and its efforts to align with global sustainability initiatives.\n",
      "\n",
      "Overall, the inclusion of environmental impact in business risk evaluation in your accounting department's curriculum reflects the sustainability standards and demonstrates the school's dedication to addressing societal impact. This update aligns with the criteria for faculty sufficiency and contributes to the continuous improvement of the school's practices. \n",
      "\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      " 2: Standard 5 specifies a systematic process for assurance of learning. What do peer review teams usually expect in determining whether this standard is met? \n",
      "\n",
      " 2: Peer review teams usually expect several key elements in determining whether Standard 5, which specifies a systematic process for assurance of learning, is met. These expectations are based on the evidence provided by the school and its programs.\n",
      "\n",
      "Firstly, the peer review teams expect the school to identify learning competencies for its business degree programs. These competencies should align with the school's mission and strategies, as well as reflect the expectations of stakeholders. The school should have a clear understanding of what knowledge, skills, and abilities students should acquire through their degree programs.\n",
      "\n",
      "Secondly, the peer review teams expect the school to assess these learning competencies using both direct and indirect measures. Direct measures may include exams, projects, or presentations that directly assess students' mastery of the identified competencies. Indirect measures may include surveys or feedback from employers or alumni that provide insights into students' performance and the relevance of their education.\n",
      "\n",
      "Furthermore, the peer review teams expect the school to demonstrate that these learning competencies are achieved across all degree programs. This means that the school should have evidence showing that students in different programs are acquiring the necessary knowledge, skills, and abilities. The assessment results should be systematically collected and analyzed to ensure consistency and reliability.\n",
      "\n",
      "In addition, the peer review teams look for a systematic assurance of learning (AOL) process with faculty participation. This means that faculty members should be actively involved in the assessment and improvement of student learning. The AOL process should be well-documented and include clear procedures for collecting and analyzing data, as well as taking actions to address any deficiencies identified.\n",
      "\n",
      "For newly launched programs, the peer review teams understand that it may take some time to establish a systematic assessment process that adequately demonstrates student learning. However, it is crucial for the school to have a robust assessment plan in place for these programs. This plan should outline how the school will gather sufficient data to demonstrate a systematic and effective process for assurance of learning.\n",
      "\n",
      "Moreover, the peer review teams expect to see evidence of curricular improvements based on the assessment results. The school should be able to show that the AOL process has led to meaningful changes in the curriculum, ensuring that the programs are continuously improving and meeting the expected learning outcomes.\n",
      "\n",
      "Overall, peer review teams expect the school to provide comprehensive evidence that demonstrates a systematic process for assurance of learning. This includes identifying and assessing learning competencies, achieving these competencies across degree programs, involving faculty in the AOL process, having a robust assessment plan for new programs, and making curricular improvements based on assessment results. By meeting these expectations, the school can ensure the quality and equivalence of its degree programs. \n",
      "\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      " 3: What are intellectual contributions \n",
      "\n",
      " 3: Intellectual contributions refer to the various outputs and contributions made by individuals within the academic and professional community. These contributions are typically categorized and evaluated based on their quality and impact. In the context of schools and educational institutions, intellectual contributions are often classified within a portfolio, which serves as a comprehensive record of an individual's work.\n",
      "\n",
      "The categorization of intellectual contributions within a portfolio can be based on different criteria, such as the level of peer or expert review that the contributions have undergone. Peer-reviewed intellectual contributions are subject to scrutiny and evaluation by others who possess recognized subject matter expertise in the field. This process ensures that the contributions meet a certain standard of quality and credibility.\n",
      "\n",
      "The portfolio of intellectual contributions can be further divided into three main parts. The first part consists of peer-reviewed journal articles, which are considered to be a significant and impactful form of intellectual contribution. The second part focuses on the production of high-quality and impactful intellectual contributions within the school or educational institution. This includes the processes, systems, and resources that support the production of such contributions. The third part involves assessing the impact of the school's intellectual contributions on theory, practice, and teaching in the business field.\n",
      "\n",
      "It is important to note that the distribution of intellectual contributions across categories in the portfolio is aligned with the school's mission, strategy, aspirations, and reputation as a thought leader. This means that the types of intellectual contributions can vary across different schools and institutions, depending on their specific focus and goals.\n",
      "\n",
      "In summary, intellectual contributions encompass a wide range of outputs and contributions made by individuals within the academic and professional community. These contributions are categorized and evaluated based on their quality and impact, with peer-reviewed journal articles being a significant component. The distribution of intellectual contributions within a portfolio is aligned with the school's mission and strategy, and the types of contributions can vary across different institutions. \n",
      "\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      " 4: Must faculty members publish in order to be qualified as scholarly academic, practice academic, or scholarly practitioner? \n",
      "\n",
      " 4: In order to be qualified as a scholarly academic, practice academic, or scholarly practitioner, faculty members must publish. The qualifications for faculty members are categorized into four categories, which demonstrate their current relevant intellectual capital and professional engagement in the area of teaching and support for the school's mission-related activities.\n",
      "\n",
      "For scholarly academics (SA), faculty members are typically required to have attained a terminal degree in their field or a related area of teaching to sustain currency and relevancy. The initial classification for SA status is based on the criteria established by the school, which includes the faculty member's initial qualifications and sustained engagement activities that support the currency and relevancy of their teaching field. SA faculty members are expected to produce peer-reviewed or editorial-reviewed publications related to their field of teaching as part of their scholarship portfolio. Additionally, they may also produce publications outside of their field of teaching to support the mission-related components of the school.\n",
      "\n",
      "Practice academics (PA) are faculty members who have attained a terminal degree in their field or a related area of teaching to sustain currency and relevancy in their professional engagement and interaction activities related to their field of teaching. PA faculty members are not required to produce publications related to their field of teaching as part of their scholarship portfolio, but they are expected to engage in sustained academic and professional activities within their area of teaching.\n",
      "\n",
      "Scholarly practitioners (SP) are faculty members who have attained a terminal degree and have sustained academic and professional engagement within their area of teaching. They follow the same criteria as scholarly academics (SA) in terms of attaining a terminal degree and sustaining currency and relevancy in their scholarship activities related to their field of teaching. SP faculty members are also expected to produce peer-reviewed or editorial-reviewed publications related to their field of teaching as part of their scholarship portfolio. Similar to SA faculty members, they may also produce publications outside of their field of teaching to support the mission-related components of the school.\n",
      "\n",
      "It is important to note that the criteria and policies for faculty qualifications, initial classification, and maintenance of qualified status are consistent with the mission of the school and comparable to peer schools. The discipline-global ratio minimums for SA faculty members are normally a minimum of 40 percent of the school's faculty resources, while SA, PA, SP, and IP faculty members should make up at least 90 percent of the faculty resources.\n",
      "\n",
      "In summary, faculty members must publish in order to be qualified as scholarly academics, practice academics, or scholarly practitioners. The specific requirements for publications and sustained engagement activities vary depending on the category of faculty qualifications, but all categories emphasize the importance of current relevant intellectual capital and professional engagement in the area of teaching. \n",
      "\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      " 5: Who is the dean of my school \n",
      "\n",
      " 5: Management courses are a part of the degree program offered by various schools. These courses are designed to provide learners with the necessary knowledge and skills to effectively manage and lead organizations. The accreditation process ensures that these courses meet the learning competencies required for a management program.\n",
      "\n",
      "During the continuous improvement review process, schools assess the effectiveness of their management courses in meeting the learning outcomes. This assessment includes evaluating the engagement of faculty in the assurance of learning (AOL) processes. The AOL processes involve evaluating learner performance and ensuring that it aligns with the expectations set by regional and country regulations, quality assurance organizations, and best practices consistent with AACSB standards.\n",
      "\n",
      "To demonstrate assurance of learning, schools need to provide evidence that their faculty is sufficiently and meaningfully engaged in the AOL processes. This includes documenting efforts made by the school to improve learning outcomes and the systems in place to support effective continuous improvement of learner performance.\n",
      "\n",
      "If you require more specific information about the management courses, I recommend consulting your mentor or the peer review team chair for guidance. They will be able to provide you with detailed information about the degree program structure and any specific requirements for degree equivalency.\n",
      "\n",
      "Please note that I don't have access to specific information about the degree program structure or the management courses offered by individual schools. It is best to consult the relevant authorities or the school directly for accurate and up-to-date information. \n",
      "\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      " 6: What are the management courses \n",
      "\n",
      " 6: In the undergraduate business program at the J. Whitney Bunting College of Business, there are several learning objectives that are assessed. These objectives are designed to ensure that students develop the necessary skills and knowledge to succeed in the business field. \n",
      "\n",
      "The learning objectives for the BBA program are as follows:\n",
      "\n",
      "1.1 Identify and evaluate ethical issues and their resolution\n",
      "1.2 Evaluate the effect of globalization and cross-culturalism in a business environment\n",
      "2.1 Apply appropriate analytical techniques in business environments\n",
      "2.2 Communicate effectively through written and oral media\n",
      "2.3 Demonstrate basic functional abilities across core business subjects\n",
      "\n",
      "To assess the first learning objective, which is to identify and evaluate ethical issues and their resolution, student learning outcomes related to ethics from the courses CBIS 2220 Principles of Information Systems and LENB 3135 Legal Environment of Business were used. The assessment results showed that students were able to correctly identify and evaluate ethical issues. However, there was a decrease in performance in LENB 3135 in the spring semester, with only 82% of students meeting or exceeding expectations compared to 97% in the fall semester. This decrease may be attributed to the nature of the ethics question, which required independent research by the students. This question will be used again in the upcoming academic year.\n",
      "\n",
      "To assess the second learning objective, which is to evaluate the effect of globalization and cross-culturalism in a business environment, specific assessment details were not provided in the context.\n",
      "\n",
      "To assess the third learning objective, which is to apply appropriate analytical techniques in business environments, two courses were used: LENB 3135 and MKTG 3161. LENB 3135 assessed how well students could discuss the laws that relate to contracts, including the UCC. The assessment results showed that in the fall semester of 14-15, 80% of students met or exceeded the target, while in the spring semester, only 62% met or exceeded the target. In the following academic year, the fall semester had 74% meeting or exceeding expectations, and the spring semester had 72% meeting or exceeding expectations. MKTG 3161 assessed how well students could identify key marketing concepts and apply them to real-world business problems. The assessment results showed that in 13-14, 68% of students met the target, and an additional 16% missed by only 1 question. In 14-15, 81% of students met or exceeded the target. The same assessment was used in 15-16, and 93% of students taking both the pre and post questions scored 80% or higher on the post questions.\n",
      "\n",
      "To assess the fourth learning objective, which is to communicate effectively through written and oral media, specific assessment details were not provided in the context.\n",
      "\n",
      "To assess the fifth learning objective, which is to demonstrate basic functional abilities across core business subjects, two courses were used: LENB 3135 and MKTG 3161. However, it was mentioned that the Undergraduate Curriculum Committee decided to use only the ETS exam in the future to assess core business knowledge, so the assessments for LENB 3135 and MKTG 3161 will not be repeated next year.\n",
      "\n",
      "In addition to these learning objectives, the BBA program also requires students to obtain common business knowledge (CBK) through the common business core courses. The assessment results for the CBK were not provided in the context.\n",
      "\n",
      "Overall, the undergraduate business program at the J. Whitney Bunting College of Business assesses various learning objectives to ensure that students develop the necessary skills and knowledge to succeed in the business field. The assessment results help identify areas of improvement and guide future curriculum development. \n",
      "\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      "======================================================\n",
      " 7: Which leaning objectives are assessed in the undergradute business program  \n",
      "\n",
      " 7: Standard 8 is defined as the impact scholarship. It focuses on the faculty's collective production of high-quality and impactful intellectual contributions over time, in order to develop areas of thought leadership that are consistent with the school's mission. \n",
      "\n",
      "Under this standard, the school also collaborates with a wide variety of external stakeholders to create and transfer credible, relevant, and timely knowledge that informs theory, policy, and practice in the business field. This collaboration is crucial in developing areas of thought leadership that align with the school's mission.\n",
      "\n",
      "The school's portfolio of intellectual contributions contains exemplars of basic and applied pedagogical research. This means that the faculty's scholarly work encompasses both theoretical and practical research that contributes to the understanding and improvement of teaching methods. \n",
      "\n",
      "It is important to note that the impact scholarship standard is closely related to standard 7, which focuses on teaching effectiveness and impact. Standard 7 ensures that the school has a systematic, multi-measure assessment process in place to ensure the quality of teaching and its impact on learner success. Additionally, the school engages in development activities to enhance faculty teaching skills and ensure that teachers deliver a curriculum that is current, relevant, forward-looking, globally-oriented, and aligned with program competency goals. \n",
      "\n",
      "Furthermore, faculty members are expected to stay current in their discipline and pedagogical methods, including teaching diverse perspectives in an inclusive environment. This commitment to lifelong learning is an essential aspect of standard 7 and contributes to the overall impact scholarship of the school.\n",
      "\n",
      "In summary, standard 8 emphasizes the importance of faculty producing high-quality and impactful intellectual contributions over time, in order to develop areas of thought leadership that align with the school's mission. This standard also highlights the collaboration with external stakeholders to create and transfer knowledge that informs theory, policy, and practice in the business field. The school's portfolio of intellectual contributions includes exemplars of basic and applied pedagogical research. Standard 8 is closely related to standard 7, which focuses on teaching effectiveness and impact, and emphasizes the importance of faculty development and lifelong learning. \n",
      "\n",
      "======================================================\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (q,o) in enumerate(zip(test_query,test_out)):\n",
    "    print(\"======================================================\")\n",
    "    print(\"======================================================\")\n",
    "    print(f\" {i}: {q} \\n\")\n",
    "    print(f\" {i}: {o} \\n\")\n",
    "    print(\"======================================================\")\n",
    "    print(\"======================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i, (q, o) in enumerate(zip(test_query, test_out)):\n",
    "    entry = {\"query\": q, \"output\": o, \"idx\": i}\n",
    "    data.append(entry)\n",
    "\n",
    "# Write data to JSON file\n",
    "with open(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_advRAG_output.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
